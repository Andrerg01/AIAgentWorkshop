{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "5e9b0b6b-e668-449d-8ec1-1c44e484495e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "_LSc2gkFbRZx"
      },
      "source": [
        "# Introduction\n",
        "Welcome to the course! Here, we’ll learn everything from simple communication with LLMs to building a complex AI Agent.\n",
        "\n",
        "First, let's address the question: what is an AI Agent?\n",
        "\n",
        "*Simple answer*: An AI Agent in an AI Assistant with extra tools. These tools can be anything you program it to be, from querying data, routing requests, looking for key words, searching the internet, running Python code, ordering pizza, you name it.\n",
        "\n",
        "This course it divided into four modules:\n",
        " - **Introduction**\n",
        "   - We'll learn about the course, the Databricks environment, and a basic way to communicate with the serving LLMs\n",
        " - **Simple Chat**\n",
        "   - We'll learn to use LangGraph to create a simple chat with an LLM, which will function as a basic AI Assistant.\n",
        " - **Introducing Tools**\n",
        "   - In this module we'll introduce the first tools to our agent. We'll learn how to build a more complex graph with them, and an agent network.\n",
        " - **Final Agent**\n",
        "   - In the final module, we'll have built a complex agent with multiple tools and functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "92dc3a1f-f0df-4735-9fc7-e8db051057cf",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "421lGuFGbRZy"
      },
      "source": [
        "# Talking to LLMs\n",
        "If you're using Databricks, go to the \"Serving\" section in the lower-left corner to view the available models. We should have enough credits to use them comfortably for the duration of the course.\n",
        "If you're using Google Colab, I'll be providing my own personal API Key to OpenAI. Please don't abuse it.\n",
        "\n",
        "For this course we'll be using the models:\n",
        " - Databricks:\n",
        "   - databricks-llama-4-maverick | Great for general chat\n",
        "   - databricks-meta-llama-3-1-8b-instruct | Great of following specific instructions\n",
        " - Google Colab:\n",
        "   - GPT 4o | General purpose model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "c5d834e2-e9f8-44af-b4fb-70f872a80a8d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "43PyDatybRZz"
      },
      "source": [
        "# 1 - Talking to LLMs - Databricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "3be43388-9415-4b3f-9465-616d95004752",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "-sogL9msbRZz"
      },
      "source": [
        "## 1.1 - Initial Configurations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "535c6fd6-dd9e-4640-91a2-c68a2a9a8b1d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "S1cFcZkIbRZz"
      },
      "source": [
        "### 1.1.1 - Installs\n",
        "\n",
        "The only package we'll need to talk to the LLMs in databricks is \"requests\", easy peasy. More stuff will come later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "4759dccd-7f9d-49e1-a2e9-06033f2826a3",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "5vPZeNyYbRZz",
        "outputId": "e92a9f9b-4f26-4101-b3b2-ec5c277baab4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.94.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "070a6596-ef9f-4c2a-ae52-bcb00747f211",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "IhwtDOW3bRZ0"
      },
      "source": [
        "### 1.1.2 - Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "30b786e0-1a85-480f-a0a6-4e348e6ae339",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "nv33TXy5bRZ0"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "e8cbafa6-2bfc-4edd-9cec-35d149413558",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "GWVBSy4tbRZ0"
      },
      "source": [
        "### 1.1.3 - Configs\n",
        "Since this course is for educational purposes, I’ll paste the token directly here—no need to overcomplicate things. However, if you plan to use it in a real application, please make sure to keep it secure and private."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "d550a403-60db-4864-8900-59e61eeb4983",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "8VDSExGibRZ1"
      },
      "outputs": [],
      "source": [
        "# Chat Model\n",
        "CHAT_ENDPOINT = \"gpt-4o\"\n",
        "# Instruct Model\n",
        "INSTRUCT_ENDPOINT = \"gpt-4o\"\n",
        "# This is my key, don't abuse it.\n",
        "OPENAI_API_KEY = \"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\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 - Basic Request - Google Colab"
      ],
      "metadata": {
        "id": "yGRy9D_IdVzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 'messages' is the parameter passed to the endpoint—it holds the chat history.\n",
        "# It's always a list of dictionaries with two keys:\n",
        "#   - \"role\" (who sent the message): \"user\", \"assistant\", or \"system\"\n",
        "#   - \"content\" (the actual message)\n",
        "# The \"assistant\" role refers to the LLM's responses.\n",
        "# We'll start with a basic interaction using a system prompt and a user message.\n",
        "\n",
        "# First we initialize the client\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# This is the chat history, same format as with Databricks: a list of dicts with role and content\n",
        "chat_history = [\n",
        "    {\"role\": \"system\", \"content\": \"You're a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you tell me a funny joke?\"}\n",
        "]\n",
        "\n",
        "# Pass the chat history and parameters like temperature and max_tokens\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o\",\n",
        "    messages=chat_history,\n",
        "    temperature=0.7,  # Controls creativity, 0 = deterministic, 1 = more creative\n",
        "    max_tokens=1000   # Maximum length of the reply\n",
        ")\n",
        "\n",
        "# Let's extract just what we need, and print it\n",
        "print(response.choices[0].message.content)\n",
        "\n",
        "# Optionally, append the assistant’s reply to the chat history\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": response.choices[0].message.content})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2v8QoiYdT6g",
        "outputId": "44e76e07-d131-420f-f966-2b38c5e3e6d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Why don't scientists trust atoms?\n",
            "\n",
            "Because they make up everything!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Below is a function that has all that mess inside of it, and returns the response as a string. Easy to use, we'll be using that on the other notebooks.\n",
        "def openai_llm(messages, model_endpoint=\"gpt-4o\", verbose=False):\n",
        "    \"\"\"\n",
        "    Calls OpenAI's chat completion endpoint.\n",
        "    Creates and destroys the client inside the function.\n",
        "    Returns the assistant's response as a string.\n",
        "    \"\"\"\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)  # Create the client\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== LLM CALL →\", model_endpoint, \" ===\")\n",
        "        for m in messages:\n",
        "            print(f\"{m['role'].upper()}: {m['content']}\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_endpoint,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    if verbose: print(\"LLM RESPONSE:\", content[:300] + (\"…\" if len(content) > 300 else \"\"))\n",
        "    if verbose: print(\"=== LLM CALL END ===\")\n",
        "\n",
        "    return content\n",
        "\n",
        "# Testing the function\n",
        "messages_test = [\n",
        "    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
        "    {\"role\":\"user\", \"content\":\"What's the capital of France?\"}\n",
        "    ]\n",
        "\n",
        "print(openai_llm(messages_test, model_endpoint=CHAT_ENDPOINT, verbose=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjpeDFXfeFJx",
        "outputId": "c4cf12d9-0e6b-4041-d066-cee08a2de408"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== LLM CALL → gpt-4o  ===\n",
            "SYSTEM: You are a helpful assistant.\n",
            "USER: What's the capital of France?\n",
            "LLM RESPONSE: The capital of France is Paris.\n",
            "=== LLM CALL END ===\n",
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QObQSYDietSa"
      },
      "execution_count": 7,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "2"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "1 - Introduction",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}