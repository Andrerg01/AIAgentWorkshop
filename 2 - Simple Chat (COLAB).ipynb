{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "a1caee55-e101-40a1-9edd-0d4c072e7ad8",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "6mDriHmcgWn2"
      },
      "source": [
        "# Simple Chat\n",
        "\n",
        "Awesome! We now know how to connect to our LLM and ask stuff from models!\n",
        "\n",
        "Now that we can do basic communication with our LLMs, we'll start on building a complex agent. Let's start with some basic configurations, learning our syntax, and establishing basic communication with an LLM through LangGraph, then we'll complicate it by adding some tools the LLM can take advantage of.\n",
        "\n",
        "Could we build a simple chat without LangGraph? Yes, easily. Could we do the rest of the course without LangGraph? Yes, but not easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9d4b0a96-728e-4b55-9090-b4534d00f76b",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "j3N7AssxgWn4"
      },
      "source": [
        "## 1 Configs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "7c409f44-99ec-4e2a-8605-19ea5ec8520c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "P0mCI_JPgWn4"
      },
      "source": [
        "### 1.1 Installs\n",
        "\n",
        " - `langgraph==0.0.36` – the graph engine that lets us wire nodes together into a stateful workflow.\n",
        "\n",
        " - `langchain (≥ 0.1.20 < 0.2.0)` – full LangChain toolkit (agents, tools, retrievers). We’ll lean on it as we grow the course.\n",
        "\n",
        " - `langchain-core (≥ 0.1.20 < 0.2.0)` – the lightweight “interfaces-only” slice of LangChain. Gives us the Runnable abstraction without all the heavy extras.\n",
        "\n",
        " - `requests` – dead-simple HTTP client. We use it once to hit the Databricks serving endpoint.\n",
        "\n",
        "After installing, we restart the kernel (dbutils.library.restartPython()) so the freshly-added packages are importable in the same notebook session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "28d0584b-eb99-4c67-8f00-732ff6fb3f5c",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QxQvIIs_gWn4",
        "outputId": "75951ab2-4a68-469e-c33e-1c54cffe82b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langgraph==0.0.36\n",
            "  Downloading langgraph-0.0.36-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m41.0/44.8 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m869.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain<0.2.0,>=0.1.20\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.20\n",
            "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.94.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.2.0,>=0.1.20) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain<0.2.0,>=0.1.20) (2.0.41)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain<0.2.0,>=0.1.20) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain<0.2.0,>=0.1.20)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.20)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.20)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain<0.2.0,>=0.1.20)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1 (from langchain<0.2.0,>=0.1.20)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain<0.2.0,>=0.1.20) (2.11.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain<0.2.0,>=0.1.20) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.2.0,>=0.1.20) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.20) (1.33)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.20)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.20) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.20) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.20) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.20) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.20) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.20) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.20) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.20)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.20)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.7.9)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.20) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.20) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.20) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.20) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.20) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.20) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.20) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.20) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.20) (3.2.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.20)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langgraph-0.0.36-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: packaging, numpy, mypy-extensions, typing-inspect, marshmallow, langsmith, dataclasses-json, langchain-core, langgraph, langchain-text-splitters, langchain-community, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.4.4\n",
            "    Uninstalling langsmith-0.4.4:\n",
            "      Successfully uninstalled langsmith-0.4.4\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.68\n",
            "    Uninstalling langchain-core-0.3.68:\n",
            "      Successfully uninstalled langchain-core-0.3.68\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.8\n",
            "    Uninstalling langchain-text-splitters-0.3.8:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.8\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.26\n",
            "    Uninstalling langchain-0.3.26:\n",
            "      Successfully uninstalled langchain-0.3.26\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-cloud-bigquery 3.34.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "db-dtypes 1.4.3 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.53 langchain-text-splitters-0.0.2 langgraph-0.0.36 langsmith-0.1.147 marshmallow-3.26.1 mypy-extensions-1.1.0 numpy-1.26.4 packaging-23.2 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "packaging"
                ]
              },
              "id": "ab087bfca57e4fff95a72ebe4085bf08"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "%pip install \"langgraph==0.0.36\" \"langchain>=0.1.20,<0.2.0\" \"langchain-core>=0.1.20,<0.2.0\" openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4f58dc9b-e566-4689-906e-d1880eea4604",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "EAwfRXnNgWn5"
      },
      "source": [
        "### 1.2 Imports\n",
        "\n",
        " - from `langgraph.graph import StateGraph, END`\n",
        "   - `StateGraph` – build and compile our node graph.\n",
        "   - `END` – sentinel that tells LangGraph where to stop.\n",
        "\n",
        " - `from langchain_core.runnables import RunnableLambda` – wraps a normal Python function so the graph can call it like any other LangChain “runnable.”\n",
        "\n",
        " - `from typing import Dict, List, Optional, TypedDict` – creates AgentState, a typed dictionary that documents (and type-checks) the keys we pass between nodes.\n",
        "\n",
        " - `import json` – handy for future pretty-printing / logging of payloads (not strictly required yet)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "bdb893fa-d502-4bc1-82fd-212bd0c1fd1f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "SIqHtLtpgWn6"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from typing import Dict, List, Optional, TypedDict\n",
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "f53b3e5c-8e1f-4a75-a391-c2659bd28a25",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "cmLROCW0gWn6"
      },
      "source": [
        "### 1.3 Config Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "331e806a-3637-47ba-aae1-3b9012cb704e",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "dAJYG0-3gWn6"
      },
      "outputs": [],
      "source": [
        "# Chat Model\n",
        "CHAT_ENDPOINT = \"gpt-4o\"\n",
        "# Instruct Model\n",
        "INSTRUCT_ENDPOINT = \"gpt-4o\"\n",
        "# This is my key, don't abuse it.\n",
        "OPENAI_API_KEY = \"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\"\n",
        "# Global toggle to see hidden outputs\n",
        "VERBOSE = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "bfbfada0-36c9-4ed9-9ca2-72961ea73a93",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "0fiWlF3tgWn6"
      },
      "source": [
        "## 2 Defining Functions and Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "fa34b46c-e292-46cb-af70-3cdb237cdb03",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "MDGfIbU8gWn6"
      },
      "source": [
        "### 2.1 Classes\n",
        "\n",
        "Now here we'll see one of the most important classes so far, the AgentState (call it whatever you'd like, ChatState, ChatFlow, State, whatever, I like AgentState).\n",
        "\n",
        "As we travel through the nodes of our complex agent, this state will carry information around. It's necessary for the LangGraph setup.\n",
        "\n",
        "On the example below, no matter where we are in the logic, the node will have access to the chat history (messages), verbosity (for our sake), and the output from the last node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "07a00e8f-b52b-4559-a947-e3a8073c6998",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "xK44Yyo7gWn6"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict, total=False):\n",
        "    chat_history: List[Dict[str, str]]  # chat history in OpenAI‑style format\n",
        "    verbose: bool                       # toggle debug prints\n",
        "    output: Optional[str]               # assistant response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ceb25bb3-e675-4c97-9cc2-5384cdcaaccd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "t3Fq4OtUgWn6"
      },
      "source": [
        "### 2.2 - Connection Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "ab6df9d7-630b-4331-a610-dd2199045c37",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "j-0keeHNgWn6"
      },
      "outputs": [],
      "source": [
        "def openai_llm(messages, model_endpoint=\"gpt-4o\", verbose=False):\n",
        "    \"\"\"\n",
        "    Calls OpenAI's chat completion endpoint.\n",
        "    Creates and destroys the client inside the function.\n",
        "    Returns the assistant's response as a string.\n",
        "    \"\"\"\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)  # Create the client\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== LLM CALL →\", model_endpoint, \" ===\")\n",
        "        for m in messages:\n",
        "            print(f\"{m['role'].upper()}: {m['content']}\")\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_endpoint,\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1000\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    if verbose: print(\"LLM RESPONSE:\", content[:300] + (\"…\" if len(content) > 300 else \"\"))\n",
        "    if verbose: print(\"=== LLM CALL END ===\")\n",
        "\n",
        "    return content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cd0730cb-acbe-4339-aec8-57fdc1626de2",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "BCLU8rX2gWn7"
      },
      "source": [
        "### 2.3 - Defining Agents and Tools as Functions\n",
        "At this point we'd define all the different tools and agents. But of course, right now we only have one agent, so it'll be simple.\n",
        "\n",
        "The agent below is expressed as a function. It takes in the AgentSate (as all tools and agents will). Sends it to the LLM, gets the response, updates the chat history, and returns the agent state again, but with the updates values for messages and output. That simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "cf9fd0f5-29e4-458b-a96c-928d3a63bd17",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "ulmNpLCXgWn7"
      },
      "outputs": [],
      "source": [
        "def chat_agent(state):\n",
        "    \"\"\"Takes current state, appends assistant reply, and returns updated state.\"\"\"\n",
        "    if state[\"verbose\"]: print(\"\\n--- CHAT AGENT NODE ---\")\n",
        "\n",
        "    # Feeds in current state's chat history and get's LLM's response\n",
        "    reply = openai_llm(\n",
        "        state[\"chat_history\"],\n",
        "        model_endpoint=CHAT_ENDPOINT,\n",
        "        verbose=state[\"verbose\"]\n",
        "    )\n",
        "\n",
        "    # Updates the chat history in the state, and the output\n",
        "    state[\"chat_history\"].append({\"role\": \"assistant\", \"content\": reply})\n",
        "    state[\"output\"] = reply\n",
        "\n",
        "    if state[\"verbose\"]: print(\"\\n--- CHAT AGENT NODE END ---\")\n",
        "\n",
        "    # Returns updated version of state\n",
        "    return state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "9a16ba96-5ec1-4d83-8752-22c1b6cac386",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "zPYA91vygWn7"
      },
      "source": [
        "## 3 Initializing Simple Chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "ce235b19-58f4-4bf2-a8d1-acebe9157a26",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "L3-p7VAXgWn7"
      },
      "source": [
        "### 3.1 - Defining Graph\n",
        "\n",
        "Here we'll define how the graph of all of our agents and tools is connected. Very simple in this case.\n",
        "\n",
        "An graph always needs one `entry point`, and `nodes`, all connected by `edges`. and (at least one) `END`\n",
        "\n",
        " - `node`: A node is any function (or Runnable) that takes an AgentState and returns an updated version of it.\n",
        " - `edge`: An edge says “when node A finishes, send the state to node B.”\n",
        " - `entry_point`: Defines which node runs first when the graph is invoked.\n",
        " - `END`: This means: when chat_agent() finishes, stop the graph here. `END` is not a node, but a signal that execution halts\n",
        "\n",
        "In our case, we have one `node`, one `edge`, and one `END`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "1802d2e0-3ba3-4927-97af-4634c5972ddd",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "u2rZra1ngWn7"
      },
      "outputs": [],
      "source": [
        "# Initializing a graph with the Agent State class\n",
        "g = StateGraph(AgentState)\n",
        "\n",
        "# Define a node for out chat agent, which will run the chat_agent function, we're not defining how it connects to anything yet.\n",
        "g.add_node(\"chat_agent\", RunnableLambda(chat_agent))\n",
        "\n",
        "# Tell the graph what's the first node to run, in this case it's the chat_agent\n",
        "g.set_entry_point(\"chat_agent\")\n",
        "# Tell the graph that once the chat_agent node is done running, end it.\n",
        "g.add_edge(\"chat_agent\", END)\n",
        "\n",
        "# Compiles the graph together\n",
        "simple_chat = g.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "40a040d8-590f-4713-9a8c-fb9d7b2018a1",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "id": "g2syybYMgWn7"
      },
      "source": [
        "### 3.2 - Basic Chat Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {},
          "inputWidgets": {},
          "nuid": "4397e75c-b8cf-4efe-8d65-07abc71c8e08",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Nx7fgT8ggWn7",
        "outputId": "3ffe1fef-9fa1-4597-d617-df54e979c4e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: good morning\n",
            "\n",
            "--- CHAT AGENT NODE ---\n",
            "\n",
            "=== LLM CALL → gpt-4o  ===\n",
            "SYSTEM: You are a helpful AI assistant that always talks like a pirate.\n",
            "USER: good morning\n",
            "LLM RESPONSE: Ahoy, matey! A fine mornin' to ye! How be the seas treatin' ye today?\n",
            "=== LLM CALL END ===\n",
            "\n",
            "--- CHAT AGENT NODE END ---\n",
            "Assistant: Ahoy, matey! A fine mornin' to ye! How be the seas treatin' ye today?\n",
            "You: exit\n"
          ]
        }
      ],
      "source": [
        "# Define some system prompt in the chat history\n",
        "chat_history = [{\"role\": \"system\", \"content\": \"You are a helpful AI assistant that always talks like a pirate.\"}]\n",
        "state = AgentState(\n",
        "    chat_history=chat_history,\n",
        "    verbose=VERBOSE,\n",
        "    output=None\n",
        ")\n",
        "\n",
        "while True:\n",
        "    # Gets the user's prompt\n",
        "    user_text = input(\"You: \").strip()\n",
        "    # Exit strategy\n",
        "    if user_text == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Append the user's message to the chat history of the state\n",
        "    state[\"chat_history\"].append({\"role\": \"user\", \"content\": user_text})\n",
        "\n",
        "    # Updates the state after going through the graph\n",
        "    state = simple_chat.invoke(state)\n",
        "\n",
        "    print(\"Assistant:\", state[\"output\"]) # Could have also called `state[\"chat_history\"][-1][\"content\"]`"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L8nPTazjqG1J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "2"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "2 - Simple Chat",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}