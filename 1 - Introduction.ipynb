{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e9b0b6b-e668-449d-8ec1-1c44e484495e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "Welcome to the course! Here, we’ll learn everything from simple communication with LLMs to building a complex AI Agent.\n",
    "\n",
    "First, let's address the question: what is an AI Agent?\n",
    "\n",
    "*Simple answer*: An AI Agent in an AI Assistant with extra tools. These tools can be anything you program it to be, from querying data, routing requests, looking for key words, searching the internet, running Python code, ordering pizza, you name it.\n",
    "\n",
    "This course it divided into four modules:\n",
    " - **Introduction**\n",
    "   - We'll learn about the course, the Databricks environment, and a basic way to communicate with the serving LLMs\n",
    " - **Simple Chat**\n",
    "   - We'll learn to use LangGraph to create a simple chat with an LLM, which will function as a basic AI Assistant.\n",
    " - **Introducing Tools**\n",
    "   - In this module we'll introduce the first tools to our agent. We'll learn how to build a more complex graph with them, and an agent network.\n",
    " - **Final Agent**\n",
    "   - In the final module, we'll have built a complex agent with multiple tools and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92dc3a1f-f0df-4735-9fc7-e8db051057cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Talking to LLMs\n",
    "If you're using Databricks, go to the \"Serving\" section in the lower-left corner to view the available models. We should have enough credits to use them comfortably for the duration of the course.\n",
    "If you're using Google Colab, I'll be providing my own personal API Key to OpenAI. Please don't abuse it.\n",
    "\n",
    "For this course we'll be using the models:\n",
    " - Databricks:\n",
    "   - databricks-llama-4-maverick | Great for general chat\n",
    "   - databricks-meta-llama-3-1-8b-instruct | Great of following specific instructions\n",
    " - Google Colab:\n",
    "   - GPT 4o | General purpose model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5d834e2-e9f8-44af-b4fb-70f872a80a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1 - Talking to LLMs - Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3be43388-9415-4b3f-9465-616d95004752",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.1 - Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "535c6fd6-dd9e-4640-91a2-c68a2a9a8b1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.1 - Installs\n",
    "\n",
    "The only package we'll need to talk to the LLMs in databricks is \"requests\", easy peasy. More stuff will come later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4759dccd-7f9d-49e1-a2e9-06033f2826a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /databricks/python3/lib/python3.11/site-packages (2.31.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests) (2023.7.22)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "070a6596-ef9f-4c2a-ae52-bcb00747f211",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.2 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30b786e0-1a85-480f-a0a6-4e348e6ae339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8cbafa6-2bfc-4edd-9cec-35d149413558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.1.3 - Configs\n",
    "Since this course is for educational purposes, I’ll paste the token directly here—no need to overcomplicate things. However, if you plan to use it in a real application, please make sure to keep it secure and private."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d550a403-60db-4864-8900-59e61eeb4983",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CHAT_ENDPOINT = \"databricks-llama-4-maverick\" # Chat Model\n",
    "INSTRUCT_ENDPOINT = \"databricks-meta-llama-3-1-8b-instruct\" # Instruct Model\n",
    "DATABRICKS_URL = \"https://dbc-864a442b-39b8.cloud.databricks.com\" # The Base URL at the top of your browser\n",
    "DATABRICKS_TOKEN = \"dapi763c08facfcf240733ac46730443c6cf\" # Your own token, to get this, to to your prfile (top right) -> Settings -> Developer -> Access Tokens (Manage) -> Generate new token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bfeb5c6-60f3-479e-98fc-36c135e2a8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1.2 - Basic Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a5a874-5a81-4f9f-8977-355e7b33b41d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n{'id': 'chatcmpl_7db16d91-9752-4f4e-9184-82565d54e913', 'object': 'chat.completion', 'created': 1752169803, 'model': 'meta-llama-4-maverick-040225', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': \"Here's one:\\n\\nWhy couldn't the bicycle stand up by itself?\\n\\n(wait for it...)\\n\\nBecause it was two-tired!\\n\\nHope that made you smile!\"}, 'finish_reason': 'stop', 'logprobs': None}], 'usage': {'prompt_tokens': 29, 'completion_tokens': 31, 'total_tokens': 60}}\nHere's one:\n\nWhy couldn't the bicycle stand up by itself?\n\n(wait for it...)\n\nBecause it was two-tired!\n\nHope that made you smile!\n"
     ]
    }
   ],
   "source": [
    "# 'messages' is the parameter passed to the endpoint—it holds the chat history.\n",
    "# It's always a list of dictionaries with two keys:\n",
    "#   - \"role\" (who sent the message): \"user\", \"assistant\", or \"system\"\n",
    "#   - \"content\" (the actual message)\n",
    "# The \"assistant\" role refers to the LLM's responses.\n",
    "# We'll start with a basic interaction using a system prompt and a user message.\n",
    "messages = [\n",
    "  {\"role\":\"system\", \"content\":\"You're a helpful AI Assistant.\"},\n",
    "  {\"role\":\"user\", \"content\":\"Can you tell me a funny joke?\"}\n",
    "]\n",
    "\n",
    "# Headers contain authorization info and content type required by the endpoint\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "    \"Content-Type\":  \"application/json\"\n",
    "}\n",
    "\n",
    "# The body includes the message history, the temperature (controls creativity), and max tokens for the response\n",
    "body = {\n",
    "    \"messages\":   messages,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\":  1000\n",
    "}\n",
    "\n",
    "# Now we can make the request\n",
    "response = requests.post(\n",
    "    f\"{DATABRICKS_URL}/serving-endpoints/{CHAT_ENDPOINT}/invocations\",\n",
    "    headers=headers,\n",
    "    json=body\n",
    "  )\n",
    "\n",
    "# The response is a raw object—let’s inspect its contents\n",
    "print(response)\n",
    "\n",
    "# Convert the response to JSON to access the content.\n",
    "# The response is stored under [\"choices\"][0][\"message\"][\"content\"].\n",
    "# By default, only one response is returned (this can be changed, but we won’t worry about it now).\n",
    "print(response.json())\n",
    "\n",
    "# Let's extract just what we need, and print it\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "# Optionally, append the assistant’s reply to the chat history\n",
    "messages.append({\"role\":\"assistant\",\"content\":response.json()[\"choices\"][0][\"message\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b24b179-94c5-4042-b7ba-769f24e9498e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n=== LLM CALL → databricks-llama-4-maverick\nSYSTEM: You are a helpful assistant.\nUSER: What's the capital of France?\nLLM RESPONSE: The capital of France is Paris.\nThe capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Below is a function that has all that mess inside of it, and returns the response as a string. Easy to use, we'll be using that on the other notebooks.\n",
    "def databricks_llm(messages, model_endpoint, verbose=False):\n",
    "    \"\"\"Call a Databricks serving endpoint that follows the OpenAI chat format.\"\"\"\n",
    "    if verbose:\n",
    "        print(\"\\n=== LLM CALL →\", model_endpoint)\n",
    "        for m in messages:\n",
    "            print(f\"{m['role'].upper()}: {m['content']}\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DATABRICKS_TOKEN}\",\n",
    "        \"Content-Type\":  \"application/json\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\":   messages,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\":  1000\n",
    "    }\n",
    "\n",
    "    resp = requests.post(f\"{DATABRICKS_URL}/serving-endpoints/{model_endpoint}/invocations\", headers=headers, json=body)\n",
    "    resp.raise_for_status()\n",
    "    content = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"LLM RESPONSE:\", content[:300] + (\"…\" if len(content) > 300 else \"\"))\n",
    "    return content\n",
    "\n",
    "# Testing the function\n",
    "messages_test = [\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "    {\"role\":\"user\", \"content\":\"What's the capital of France?\"}\n",
    "    ]\n",
    "\n",
    "print(databricks_llm(messages_test, model_endpoint=CHAT_ENDPOINT, verbose=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ccf4103-593f-42cc-bfb1-8d5dfecb3614",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1 - Introduction",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}