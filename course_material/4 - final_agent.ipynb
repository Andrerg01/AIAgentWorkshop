{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8937734",
   "metadata": {},
   "source": [
    "# Final Agent\n",
    "Okay, now we're cooking. Our agent can access some data, that's super cool! But it's not the best now, is it? It can access data, but it loads the ENTIRE dataset into the chat! That's not practical. Especially if our dataset as very large.\n",
    "\n",
    "What we'll do next is to QUERY data SMARTLY! We'll have agents write the SQL to query our data for us. And that will be the final form of our agent.\n",
    "\n",
    "---\n",
    "\n",
    "In this module, we'll make up some large mock dataset, and have the agent query from that as necessary with the SQL query that seems appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9de7f",
   "metadata": {},
   "source": [
    "## 1 Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82797b",
   "metadata": {},
   "source": [
    "### 1.1 Installs\n",
    "\n",
    "* `dotenv` – loads and manages environment variables from a `.env` file.\n",
    "\n",
    "* `duckdb` – runs fast SQL queries directly on Pandas DataFrames or local files.\n",
    "\n",
    "* `langchain-core` – the lightweight core of LangChain, providing the `Runnable` interfaces and core abstractions.\n",
    "\n",
    "* `langchain_mcp_adapters` – bridges LangChain with MCP servers, letting agents call remote tools securely. (Only needed if dealing with MCP Servers)\n",
    "\n",
    "* `langgraph` – builds stateful agent workflows as directed graphs of nodes and edges.\n",
    "\n",
    "* `pandas` – flexible, high-performance data analysis and manipulation library.\n",
    "\n",
    "* `requests` – simple HTTP client for making API calls (e.g., to Databricks endpoints).\n",
    "\n",
    "* `truststore` – ensures Python’s SSL connections use your system’s trusted certificate store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9147d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "   dotenv \\\n",
    "   duckdb \\\n",
    "  \"langchain-core==0.3.79\" \\\n",
    "  \"langchain_mcp_adapters==0.1.11\" \\\n",
    "  \"langgraph==0.2.41\" \\\n",
    "  pandas \\\n",
    "  requests \\\n",
    "  truststore \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b0a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel to use newly installed packages\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8814d21c",
   "metadata": {},
   "source": [
    "### 1.2 Imports\n",
    "\n",
    " - from `langgraph.graph import StateGraph, END`\n",
    "   - `StateGraph` - build and compile our node graph.\n",
    "   - `END` - sentinel that tells LangGraph where to stop.\n",
    "\n",
    " - `from langchain_core.runnables import RunnableLambda` - wraps a normal Python function so the graph can call it like any other LangChain “runnable.”\n",
    "\n",
    " - `from typing import Dict, List, Optional, TypedDict` - creates AgentState, a typed dictionary that documents (and type-checks) the keys we pass between nodes.\n",
    "\n",
    " - `my_functions.databricks_llm` - Our custom useful function to make llm requests\n",
    "\n",
    " - `os` - Access environment variables that store our Azure configuratio\n",
    "\n",
    " - `dotenv.load_dotenv` - Reads the `.env` file and safely loads the environmental variables\n",
    "\n",
    " - `json` - Package to manage json-style strings/objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph core components for building agent workflows\n",
    "from langgraph.graph import StateGraph, END       # StateGraph: main graph builder, END: termination signal\n",
    "from langchain_core.runnables import RunnableLambda  # Converts functions to graph-compatible nodes\n",
    "\n",
    "# Type annotations for better code clarity and IDE support\n",
    "from typing import Dict, List, Optional, TypedDict\n",
    "\n",
    "# Utility and system packages\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Our custom utility functions from the previous notebook\n",
    "from my_functions import databricks_llm\n",
    "\n",
    "# Dataframe handling functions\n",
    "import pandas as pd\n",
    "import duckdb as ddb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682cb800",
   "metadata": {},
   "source": [
    "### 1.3 Loading Environmental Variables\n",
    "\n",
    "Load our Azure OpenAI configuration from the secure `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde01852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (Azure endpoints, API versions, etc.)\n",
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a097f8e",
   "metadata": {},
   "source": [
    "## 2 Defining Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12380436",
   "metadata": {},
   "source": [
    "### 2.1 Classes\n",
    "\n",
    "Here, let's change the `AgentState` class just a little mode:\n",
    " - We'll just add the field `schema_catalog`, so any node of the graph can consult it if needed. This should hold the schema of our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c14211",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    Enhanced state container for our tool-enabled agent.\n",
    "    \n",
    "    Carries conversation data and tool information between graph nodes.\n",
    "    \"\"\"\n",
    "    # Core fields for tools and conversation (from previous notebook)\n",
    "    chat_history: List[Dict[str, str]]        # Complete conversation in OpenAI format\n",
    "    output: Optional[str]                     # Most recent response\n",
    "    available_tools: Optional[Dict[str, str]] # Tool names → descriptions for router\n",
    "    tool_context: Optional[str]               # Context retrieved by tools (cleared each turn)\n",
    "\n",
    "    # New fields for database communication\n",
    "    schema_catalog: Optional[Dict[str, str]] # catalog of schemas and their descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796a3f8",
   "metadata": {},
   "source": [
    "### 2.2 Agent Functions with Tool Support\n",
    "\n",
    "**Three Specialized Agent Functions:**\n",
    "\n",
    "**1. Router Agent (`router_agent`)**\n",
    "- **Purpose**: Decides which tool to use based on user's query\n",
    "- **Input**: Chat history + available tools\n",
    "- **Output**: JSON decision like `{\"tool\": \"email\"}` or `{\"tool\": \"chat\"}`\n",
    "- **Key Feature**: Uses its own system prompt to focus on tool selection\n",
    "\n",
    "**2. Sales Data Agent (`sales_data_agent`)**  \n",
    "- **Purpose**: Looks at chat history and writes SQL query to answer user's question\n",
    "- **Process**: Adds results to `tool_context`\n",
    "\n",
    "**3. Sales Data Tool (`sales_data_tool`)**\n",
    "- **Purpose**: Runs the SQL query created by the sales_data_agent\n",
    "- **Process**: Uses duckdb to run the SQL query on the databased loaded by pandas\n",
    "- **No LLM needed**: Pure data retrieval/querying\n",
    "\n",
    "**4. Chat Agent (`chat_agent`)**\n",
    "- **Purpose**: Generates final response using conversation + tool context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd9508e",
   "metadata": {},
   "source": [
    "#### 2.2.1 - Router Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc23a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_agent(state):\n",
    "    \"\"\"\n",
    "    Router agent that decides which tool to use based on the user's query.\n",
    "    \n",
    "    This agent analyzes the conversation and available tools to make an \n",
    "    intelligent routing decision.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ROUTER AGENT NODE ---\")\n",
    "    print(f\"Analyzing user query with {len(state.get('available_tools', {}))} available tools\")\n",
    "\n",
    "    # === BUILD TOOL CATALOG FOR LLM ===\n",
    "    # Create a formatted list of available tools and their descriptions\n",
    "    tool_lines = [\n",
    "        f\"- {name}: {desc}\"\n",
    "        for name, desc in (state[\"available_tools\"] or {}).items()\n",
    "    ]\n",
    "    tool_catalog = \"\\n\".join(tool_lines) or \"none\"\n",
    "\n",
    "    # === CREATE ROUTER-SPECIFIC SYSTEM PROMPT ===\n",
    "    # The router has a specialized role: tool selection only\n",
    "    router_system_prompt = (\n",
    "        \"You are an AI router. Choose the single best tool for answering the user's \"\n",
    "        \"latest message.\\n\\n\"\n",
    "        f\"Available tools:\\n{tool_catalog}\\n\\n\"\n",
    "        \"Return ONLY a JSON object like {\\\"tool\\\": \\\"chat\\\"} or {\\\"tool\\\": \\\"sales_data\\\"}.\"\n",
    "    )\n",
    "\n",
    "    # === PREPARE MODIFIED CHAT HISTORY ===\n",
    "    # Replace system prompts with router-specific prompt\n",
    "    # This ensures the LLM focuses on tool selection, not general chat\n",
    "    modified_chat_history = [\n",
    "        {\"role\": \"system\", \"content\": router_system_prompt}\n",
    "    ] + [\n",
    "        m for m in state[\"chat_history\"] \n",
    "        if m[\"role\"] != \"system\"  # Filter out original system prompts\n",
    "    ]\n",
    "\n",
    "    print(f\"Sending {len(modified_chat_history)} messages to LLM for routing decision\")\n",
    "\n",
    "    # === GET ROUTING DECISION FROM LLM ===\n",
    "    llm_response = databricks_llm(modified_chat_history, os.getenv(\"INSTRUCT_ENDPOINT\"))\n",
    "    \n",
    "    print(f\"LLM routing response: {llm_response}\")\n",
    "\n",
    "    # === EXTRACT JSON DECISION ===\n",
    "    # Parse JSON from LLM response (handle any extra text)\n",
    "    start = llm_response.rfind(\"{\")\n",
    "    end   = llm_response.rfind(\"}\")\n",
    "    \n",
    "    if start == -1 or end == -1:\n",
    "        # Fallback to chat if no valid JSON found\n",
    "        decision = {\"tool\": \"chat\"}\n",
    "        print(\"⚠️  No valid JSON found, defaulting to chat\")\n",
    "    else:\n",
    "        decision_json = llm_response[start : end + 1]\n",
    "        try:\n",
    "            decision = json.loads(decision_json)\n",
    "            print(f\"✓ Extracted decision: {decision}\")\n",
    "        except json.JSONDecodeError:\n",
    "            decision = {\"tool\": \"chat\"}\n",
    "            print(\"⚠️  JSON parse error, defaulting to chat\")\n",
    "\n",
    "    # === UPDATE STATE ===\n",
    "    # Store decision as JSON string for conditional edges\n",
    "    state[\"output\"] = json.dumps(decision)\n",
    "\n",
    "    print(\"--- ROUTER AGENT NODE END ---\\n\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd904b",
   "metadata": {},
   "source": [
    "#### 2.2.2 - Sales Data Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d24aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_data_agent(state):\n",
    "    \"\"\"\n",
    "    Sales Data agent that writes SQL to answer user questions against the sales schema.\n",
    "\n",
    "    This agent:\n",
    "      1) Injects a schema-aware system prompt,\n",
    "      2) Strips other system prompts from history,\n",
    "      3) Calls the LLM to produce a single SQL query,\n",
    "      4) Robustly extracts a fenced SQL block,\n",
    "      5) Applies light guardrails (ensure SELECT, remove trailing semicolons),\n",
    "      6) Stores the final SQL in state[\"output\"].\n",
    "    \"\"\"\n",
    "    print(\"\\n--- SALES DATA AGENT NODE ---\")\n",
    "\n",
    "    # === LOAD SCHEMA ===\n",
    "    # Expecting a schema catalog shaped like: {\"sales_data\": \"...DDL and table/column docs...\"}\n",
    "    schema_catalog = state.get(\"schema_catalog\", {}) or {}\n",
    "    schema = schema_catalog.get(\"sales_data\")\n",
    "    if not schema:\n",
    "        print(\"⚠️  No sales_data schema found; falling back to minimal instruction.\")\n",
    "        schema = \"/* Schema unavailable: write a best-effort ANSI SQL query. */\"\n",
    "\n",
    "    # === BUILD SALES-SPECIFIC SYSTEM PROMPT ===\n",
    "    sales_system_prompt = (\n",
    "        \"You are a focused SQL authoring assistant for SALES data.\\n\"\n",
    "        \"Goal: write ONE SQL query that answers the user's latest request using the schema below.\\n\\n\"\n",
    "        f\"Schema:\\n{schema}\\n\\n\"\n",
    "        \"Constraints:\\n\"\n",
    "        \"- Output ONLY a single SQL query, no prose.\\n\"\n",
    "        \"- Wrap it exactly like:\\n\"\n",
    "        \"```sql\\nSELECT ...\\n```\\n\"\n",
    "    )\n",
    "\n",
    "    # === PREPARE MODIFIED CHAT HISTORY ===\n",
    "    # Replace any prior system prompts with our sales-specific instructions\n",
    "    modified_chat_history = (\n",
    "        [{\"role\": \"system\", \"content\": sales_system_prompt}] +\n",
    "        [m for m in state.get(\"chat_history\", []) if m.get(\"role\") != \"system\"]\n",
    "    )\n",
    "    print(f\"Sending {len(modified_chat_history)} messages to LLM\")\n",
    "\n",
    "    # === CALL LLM ===\n",
    "    llm_response = databricks_llm(\n",
    "        modified_chat_history,\n",
    "        model_endpoint=os.getenv(\"CHAT_ENDPOINT\")\n",
    "    )\n",
    "    print(\"Raw LLM response received.\")\n",
    "\n",
    "    # === EXTRACT FENCED SQL BLOCK ===\n",
    "    # Look for the last fenced block to be resilient to accidental extra text.\n",
    "    start = llm_response.rfind(\"```\")\n",
    "    end = len(llm_response)\n",
    "    # If we found a closing fence earlier, prefer the last full triple-backtick pair:\n",
    "    if start != -1:\n",
    "        # Find the opening of that block\n",
    "        open_start = llm_response.rfind(\"```\", 0, start)\n",
    "        if open_start != -1:\n",
    "            code_block = llm_response[open_start + 3:start].strip()\n",
    "        else:\n",
    "            # Only one fence found—fall back to substring after fence\n",
    "            code_block = llm_response[start + 3:].strip()\n",
    "    else:\n",
    "        # No fences—fallback: try to salvage any SELECT ...; looking substring\n",
    "        print(\"⚠️  No fenced code block found; attempting fallback extraction.\")\n",
    "        code_block = llm_response.strip()\n",
    "\n",
    "    # Remove optional \"sql\" language tag at the top of the fenced block\n",
    "    if code_block.lower().startswith(\"sql\"):\n",
    "        code_block = code_block[3:].strip()\n",
    "\n",
    "    # === LIGHT GUARDRAILS ===\n",
    "    sql = code_block.strip()\n",
    "    # If the block still contains fencing, strip again\n",
    "    if sql.startswith(\"```\") and sql.endswith(\"```\"):\n",
    "        sql = sql[3:-3].strip()\n",
    "\n",
    "    # Require a SELECT to reduce the chance of non-query outputs\n",
    "    if \"select\" not in sql.lower():\n",
    "        print(\"⚠️  No SELECT found; defaulting to a safe placeholder query.\")\n",
    "        sql = \"SELECT 1 /* fallback: LLM did not produce a valid SELECT */\"\n",
    "\n",
    "    # Normalize: remove trailing semicolon for consistency in some executors\n",
    "    sql = sql.rstrip().rstrip(\";\").strip()\n",
    "\n",
    "    print(\"Generated SQL:\\n\", sql)\n",
    "\n",
    "    # === UPDATE STATE ===\n",
    "    state[\"output\"] = sql\n",
    "\n",
    "    print(\"--- SALES DATA AGENT NODE END ---\\n\")\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb40ea",
   "metadata": {},
   "source": [
    "#### 2.2.3 - Sales Data Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e523ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_data_tool(state):\n",
    "    \"\"\"\n",
    "    Sales Data Tool Node\n",
    "\n",
    "    This tool executes the SQL query generated by the Sales Data Agent\n",
    "    against a local in-memory dataset using DuckDB. It converts the result\n",
    "    into a readable text context for downstream LLM processing.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- SALES DATA TOOL NODE ---\")\n",
    "\n",
    "    # === EXTRACT SQL QUERY FROM STATE ===\n",
    "    # The previous agent node is expected to store its output SQL here\n",
    "    query = state.get(\"output\", \"\").strip()\n",
    "    if not query:\n",
    "        print(\"⚠️  No SQL query found in state; skipping execution.\")\n",
    "        state[\"tool_context\"] = \"/* No SQL query to execute */\"\n",
    "        return state\n",
    "\n",
    "    # === LOAD DATA ===\n",
    "    # Load the mock dataset from CSV into a DataFrame\n",
    "    try:\n",
    "        sales_data = pd.read_csv(\"data/mock_sales_data.csv\")\n",
    "        print(f\"Loaded {len(sales_data):,} rows from mock_sales_data.csv\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load data: {e}\")\n",
    "        state[\"tool_context\"] = f\"/* Data load error: {e} */\"\n",
    "        return state\n",
    "\n",
    "    # === EXECUTE SQL USING DUCKDB ===\n",
    "    print(\"Executing SQL with DuckDB:\\n\", query)\n",
    "    try:\n",
    "        # Run query against in-memory DataFrame\n",
    "        result_df = ddb.query_df(sales_data, \"sales_data\", query).to_df()\n",
    "        print(f\"✓ Query executed successfully, returned {len(result_df):,} rows.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Query execution failed: {e}\")\n",
    "        result_df = pd.DataFrame({\"error\": [str(e)]})\n",
    "\n",
    "    # === PREPARE CONTEXT STRING FOR LLM ===\n",
    "    # Convert the DataFrame to a human-readable string (no index)\n",
    "    context = \"QUERY: \" + query + \"\\nANSWER: \" + result_df.to_string(index=False)\n",
    "    state[\"tool_context\"] = context\n",
    "    state[\"output\"] = context\n",
    "\n",
    "    print(\"SQL Result:\\n\", context)\n",
    "\n",
    "    print(\"--- SALES DATA TOOL NODE END ---\\n\")\n",
    "\n",
    "    # Return updated state for next node\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e7d2a",
   "metadata": {},
   "source": [
    "#### 2.2.4 - Chat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00aae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_agent(state):\n",
    "    \"\"\"\n",
    "    Enhanced chat agent that incorporates tool context into responses.\n",
    "    \n",
    "    This agent generates the final response using both the conversation\n",
    "    history and any additional context provided by tools.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- CHAT AGENT NODE ---\")\n",
    "    \n",
    "    # === PREPARE ENHANCED CHAT HISTORY ===\n",
    "    # Start with the original conversation\n",
    "    enhanced_chat_history = state[\"chat_history\"].copy()\n",
    "    \n",
    "    # === ADD TOOL CONTEXT IF AVAILABLE ===\n",
    "    if state.get(\"tool_context\"):\n",
    "        print(\"✓ Including tool context in conversation\")\n",
    "        # Add tool context as a user message so the AI can reference it\n",
    "        context_message = {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"ADDITIONAL CONTEXT FROM TOOLS:\\n{state['tool_context']}\"\n",
    "        }\n",
    "        enhanced_chat_history.append(context_message)\n",
    "    else:\n",
    "        print(\"No tool context available\")\n",
    "\n",
    "    print(f\"Sending {len(enhanced_chat_history)} messages to LLM\")\n",
    "    \n",
    "    # === GENERATE RESPONSE ===\n",
    "    reply = databricks_llm(enhanced_chat_history, os.getenv(\"CHAT_ENDPOINT\"))\n",
    "    \n",
    "    # === UPDATE STATE ===\n",
    "    # Add only the actual AI response to the permanent chat history\n",
    "    # (Don't include the temporary tool context message)\n",
    "    state[\"chat_history\"].append({\"role\": \"assistant\", \"content\": reply})\n",
    "    state[\"output\"] = reply\n",
    "\n",
    "    print(f\"✓ Generated response: {reply[:100]}...\")\n",
    "    print(\"--- CHAT AGENT NODE END ---\\n\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe2e030",
   "metadata": {},
   "source": [
    "# 3 - Initializing Chat\n",
    "\n",
    "Now we'll create the graph for the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdf778",
   "metadata": {},
   "source": [
    "### 3.1 Graph with Conditional Routing, Agents, and Tool\n",
    "\n",
    "**Our Graph Flow:**\n",
    "```\n",
    "START → router_agent → [decision] → sales_data_agent → sales_data_tool → chat_agent → END\n",
    "                           ↓\n",
    "                       chat_agent → END\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773939bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: INITIALIZE GRAPH ===\n",
    "g = StateGraph(AgentState)\n",
    "print(\"✓ Graph initialized with enhanced AgentState\")\n",
    "\n",
    "# === STEP 2: ADD NODES ===\n",
    "# Add all nodes: router, sales SQL author, SQL executor, and final chat responder\n",
    "g.add_node(\"router_agent\",      RunnableLambda(router_agent))       # Decision maker\n",
    "g.add_node(\"sales_data_agent\",  RunnableLambda(sales_data_agent))   # SQL author\n",
    "g.add_node(\"sales_data_tool\",   RunnableLambda(sales_data_tool))    # SQL executor\n",
    "g.add_node(\"chat_agent\",        RunnableLambda(chat_agent))         # Response generator\n",
    "print(\"✓ Added 4 nodes: router_agent, sales_data_agent, sales_data_tool, chat_agent\")\n",
    "\n",
    "# === STEP 3: SET ENTRY POINT ===\n",
    "# Always start with the router to choose the best path\n",
    "g.set_entry_point(\"router_agent\")\n",
    "print(\"✓ Set router_agent as entry point\")\n",
    "\n",
    "# === STEP 4: DEFINE ROUTE SELECTOR FUNCTION ===\n",
    "def route_selector(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Parse the router's decision and return the next node name.\n",
    "\n",
    "    Args:\n",
    "        state: Current agent state containing router's JSON decision in state[\"output\"]\n",
    "\n",
    "    Returns:\n",
    "        str: Node key (\"chat\" or \"sales_data\"); defaults to \"chat\" on errors.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        decision = json.loads(state[\"output\"]) if state.get(\"output\") else {}\n",
    "        tool_choice = decision.get(\"tool\", \"chat\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  Route selector failed to parse decision ({e}); defaulting to 'chat'\")\n",
    "        tool_choice = \"chat\"\n",
    "\n",
    "    print(f\"Route selector: directing to '{tool_choice}'\")\n",
    "    return tool_choice\n",
    "\n",
    "# === STEP 5: ADD CONDITIONAL EDGES ===\n",
    "# Router directs either to chat or the sales data SQL authoring agent\n",
    "g.add_conditional_edges(\n",
    "    \"router_agent\",        # Source node\n",
    "    route_selector,        # Function that decides the route\n",
    "    {\n",
    "        \"chat\":       \"chat_agent\",        # Simple questions → chat\n",
    "        \"sales_data\": \"sales_data_agent\",  # Data questions → sales SQL author\n",
    "    },\n",
    ")\n",
    "print(\"✓ Added conditional edges from router_agent\")\n",
    "\n",
    "# === STEP 6: ADD SIMPLE EDGES ===\n",
    "# Sales author → Sales tool → Chat → END\n",
    "g.add_edge(\"sales_data_agent\", \"sales_data_tool\")\n",
    "print(\"✓ Added edge: sales_data_agent → sales_data_tool\")\n",
    "\n",
    "g.add_edge(\"sales_data_tool\", \"chat_agent\")\n",
    "print(\"✓ Added edge: sales_data_tool → chat_agent\")\n",
    "\n",
    "g.add_edge(\"chat_agent\", END)\n",
    "print(\"✓ Added edge: chat_agent → END\")\n",
    "\n",
    "# === STEP 7: COMPILE THE GRAPH ===\n",
    "assistant_graph = g.compile()\n",
    "print(\"✓ Graph compiled successfully!\")\n",
    "\n",
    "print(\"\\nGraph structure:\")\n",
    "print(\"START → router_agent → [decision]\")\n",
    "print(\"                         ├─ 'chat'       → chat_agent → END\")\n",
    "print(\"                         └─ 'sales_data' → sales_data_agent → sales_data_tool → chat_agent → END\")\n",
    "print(\"\\nThe tool-enabled agent is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca561e7",
   "metadata": {},
   "source": [
    "### 3.2 Interactive Smart Querying Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f8f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INITIALIZE ENHANCED CONVERSATION STATE ===\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI Agent. You have access to a sales data database if needed.\"}\n",
    "]\n",
    "\n",
    "schema_catalog = {\n",
    "    \"sales_data\": (\n",
    "        \"The 'sales_data' table has the following columns:\\n\"\n",
    "        \"- client (string): the customer name\\n\"\n",
    "        \"- product (string): the item sold\\n\"\n",
    "        \"- quantity (int): number of units sold\\n\"\n",
    "        \"- price (float): price per unit\\n\"\n",
    "        \"- date (datetime): date of transaction\\n\"\n",
    "    )\n",
    "}\n",
    "\n",
    "state = AgentState(\n",
    "    chat_history=chat_history,\n",
    "    output=None,\n",
    "    # Define available tools for the router to choose from\n",
    "    available_tools={\n",
    "        \"chat\": \"Continue the conversation naturally\",\n",
    "        \"sales_data\": \"Query the internal sales database using SQL\"\n",
    "    },\n",
    "    tool_context=None,  # Will be populated by tools when needed\n",
    "    schema_catalog=schema_catalog\n",
    ")\n",
    "\n",
    "print(\"🤖 Tool-enabled AI Agent initialized!\")\n",
    "print(\"Available tools: sales_data search, regular chat\")\n",
    "print(\"Type 'exit' to quit the conversation.\\n\")\n",
    "\n",
    "# === MAIN ENHANCED CHAT LOOP ===\n",
    "while True:\n",
    "    # === GET USER INPUT ===\n",
    "    user_text = input(\"You: \").strip()\n",
    "    \n",
    "    # === CHECK FOR EXIT ===\n",
    "    if user_text.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    if not user_text:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n📝 Processing: '{user_text}'\")\n",
    "    \n",
    "    # === ADD USER MESSAGE TO CONVERSATION ===\n",
    "    state[\"chat_history\"].append({\"role\": \"user\", \"content\": user_text})\n",
    "    \n",
    "    # === EXECUTE THE ENHANCED GRAPH ===\n",
    "    # This will:\n",
    "    # 1. Route through router_agent to decide on tools\n",
    "    # 2. Optionally use sales_data_agent + sales_data_tool to get context\n",
    "    # 3. Generate final response with chat_agent\n",
    "    print(\"🔄 Running enhanced agent graph...\")\n",
    "    state = assistant_graph.invoke(state)\n",
    "    \n",
    "    # === CLEANUP TOOL CONTEXT ===\n",
    "    # Clear tool context after each interaction to prevent carryover\n",
    "    state[\"tool_context\"] = None\n",
    "    \n",
    "    # === DISPLAY RESPONSE ===\n",
    "    print(f\"🤖 Assistant: {state['output']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf9143d",
   "metadata": {},
   "source": [
    "## 4 - A Nod to MCP\n",
    "\n",
    "In the same spirit as the previous notebook, I'll add an example using MCP.\n",
    "\n",
    "This is an even better application, where the heavy-duty SQL queries can happen on remote databases that are much larger and might required better methods than `duckdb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec74d9c",
   "metadata": {},
   "source": [
    "### 4.1 Imports\n",
    "\n",
    "We'll put it all nice and neat in the `my_mcp_functions.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f997ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_mcp_functions import list_tools, call_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7886c5",
   "metadata": {},
   "source": [
    "### 4.2 Incorporating MCP Into our Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that it's \"async\" now, as it has API calls\n",
    "async def mcp_sales_data_tool(state):\n",
    "    print(\"\\n--- MCP SALES DATA TOOL NODE ---\")\n",
    "\n",
    "    query = state.get(\"output\", \"\").strip()\n",
    "    if not query:\n",
    "        print(\"⚠️  No SQL query found in state; skipping execution.\")\n",
    "        state[\"tool_context\"] = None\n",
    "        return state\n",
    "\n",
    "    result = await call_tool(\"query_sales_data\", {'sql_query': query})\n",
    "\n",
    "    context = \"QUERY: \" + query + \"\\nANSWER: \" + result\n",
    "\n",
    "    state[\"tool_context\"] = context\n",
    "    state[\"output\"] = context\n",
    "\n",
    "    print(\"SQL Result:\\n\", context)\n",
    "\n",
    "    print(\"--- MCP SALES DATA TOOL NODE END ---\\n\")\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ca734",
   "metadata": {},
   "source": [
    "### 4.3 Redefining Graph with New Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c54eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = StateGraph(AgentState)\n",
    "g.add_node(\"router_agent\",      RunnableLambda(router_agent))       \n",
    "g.add_node(\"sales_data_agent\",  RunnableLambda(sales_data_agent))\n",
    "g.add_node(\"sales_data_tool\",   RunnableLambda(mcp_sales_data_tool)) # <--- New Tool in\n",
    "g.add_node(\"chat_agent\",        RunnableLambda(chat_agent))\n",
    "\n",
    "g.set_entry_point(\"router_agent\")\n",
    "\n",
    "def route_selector(state: AgentState) -> str:\n",
    "    try:\n",
    "        decision = json.loads(state[\"output\"]) if state.get(\"output\") else {}\n",
    "        tool_choice = decision.get(\"tool\", \"chat\")\n",
    "    except Exception as e:\n",
    "        tool_choice = \"chat\"\n",
    "    return tool_choice\n",
    "\n",
    "g.add_conditional_edges(\n",
    "    \"router_agent\",\n",
    "    route_selector,\n",
    "    {\n",
    "        \"chat\":       \"chat_agent\",\n",
    "        \"sales_data\": \"sales_data_agent\",\n",
    "    },\n",
    ")\n",
    "g.add_edge(\"sales_data_agent\", \"sales_data_tool\")\n",
    "g.add_edge(\"sales_data_tool\", \"chat_agent\")\n",
    "g.add_edge(\"chat_agent\", END)\n",
    "\n",
    "assistant_graph = g.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bb52f",
   "metadata": {},
   "source": [
    "### 4.4 Chat Loop with MCP Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44716b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI Agent. You have access to a sales data database if needed.\"}\n",
    "]\n",
    "\n",
    "schema_catalog = {\n",
    "    \"sales_data\": (\n",
    "        \"The 'sales_data' table has the following columns:\\n\"\n",
    "        \"- client (string): the customer name\\n\"\n",
    "        \"- product (string): the item sold\\n\"\n",
    "        \"- quantity (int): number of units sold\\n\"\n",
    "        \"- price (float): price per unit\\n\"\n",
    "        \"- date (datetime): date of transaction\\n\"\n",
    "    )\n",
    "}\n",
    "\n",
    "state = AgentState(\n",
    "    chat_history=chat_history,\n",
    "    output=None,\n",
    "    available_tools={\n",
    "        \"chat\": \"Continue the conversation naturally\",\n",
    "        \"sales_data\": \"Query the internal sales database using SQL\"\n",
    "    },\n",
    "    tool_context=None,\n",
    "    schema_catalog=schema_catalog\n",
    ")\n",
    "\n",
    "print(\"🤖 Tool-enabled AI Agent initialized!\")\n",
    "print(\"Available tools: sales_data search, regular chat\")\n",
    "print(\"Type 'exit' to quit the conversation.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"You: \").strip()\n",
    "    \n",
    "    if user_text.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    if not user_text:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n📝 Processing: '{user_text}'\")\n",
    "    state[\"chat_history\"].append({\"role\": \"user\", \"content\": user_text})\n",
    "    print(\"🔄 Running enhanced agent graph...\")\n",
    "    state = await assistant_graph.ainvoke(state) # <--- Only change for asynchronous execution\n",
    "    \n",
    "    state[\"tool_context\"] = None\n",
    "    \n",
    "    print(f\"🤖 Assistant: {state['output']}\")\n",
    "    print(\"-\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
