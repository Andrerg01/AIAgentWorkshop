{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871e95ae",
   "metadata": {},
   "source": [
    "# Introducing Tools\n",
    "\n",
    "Great! Our chatbot has the basics of inner workings. But that's boring, so let's go and give it some tools! Tools are what differentiates an AI Assistant to an AI Agent.\n",
    "Let's keep it simple for this example, but you can make it as complex as you'd like.\n",
    "\n",
    "Here, we'll define a variable that holds in all of my super secret emails, and the agent will be able to acces that *when it decides it's proper*. That's the magic, the tool is always there, and if the agent feels necessary, it will call on it.\n",
    "\n",
    "---\n",
    "In this module specifically, we'll build an agent that, firstly goes thorugh a `router_agent` and decides on what to do, does it need to access the provided `emails`, or just go straight into a `chat` function?\n",
    "\n",
    "If it decides that it needs to access the `emails`, it will direct the graph to the `email_tool`, which will add the necessary context to the agent state, and then go to chat for the completion of the prompt\n",
    "\n",
    "If not, it just goes straight to the chat and completes the prompt.\n",
    "\n",
    "Let's see how we can do it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2094ea",
   "metadata": {},
   "source": [
    "## 1. Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b9b29",
   "metadata": {},
   "source": [
    "### 1.1 Installs\n",
    "\n",
    "* `dotenv` – loads and manages environment variables from a `.env` file.\n",
    "\n",
    "* `langchain-core` – the lightweight core of LangChain, providing the `Runnable` interfaces and core abstractions.\n",
    "\n",
    "* `langchain_mcp_adapters` – bridges LangChain with MCP servers, letting agents call remote tools securely. (Only needed if dealing with MCP Servers)\n",
    "\n",
    "* `langgraph` – builds stateful agent workflows as directed graphs of nodes and edges.\n",
    "\n",
    "* `requests` – simple HTTP client for making API calls (e.g., to Databricks endpoints).\n",
    "\n",
    "* `truststore` – ensures Python’s SSL connections use your system’s trusted certificate store. (Also only needed if dealing with MCP Servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "   dotenv \\\n",
    "  \"langchain-core==0.3.79\" \\\n",
    "  \"langchain_mcp_adapters==0.1.11\" \\\n",
    "  \"langgraph==0.2.41\" \\\n",
    "  requests \\\n",
    "  truststore \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel to use newly installed packages\n",
    "# dbutils.library.restartPython() # Databricks\n",
    "%reset -f # VSCode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9654e002",
   "metadata": {},
   "source": [
    "### 1.2 Imports\n",
    "\n",
    " - from `langgraph.graph import StateGraph, END`\n",
    "   - `StateGraph` - build and compile our node graph.\n",
    "   - `END` - sentinel that tells LangGraph where to stop.\n",
    "\n",
    " - `from langchain_core.runnables import RunnableLambda` - wraps a normal Python function so the graph can call it like any other LangChain “runnable.”\n",
    "\n",
    " - `from typing import Dict, List, Optional, TypedDict` - creates AgentState, a typed dictionary that documents (and type-checks) the keys we pass between nodes.\n",
    "\n",
    " - `my_functions` - Our custom useful functions\n",
    "\n",
    " - `os` - Access environment variables that store our Azure configuratio\n",
    "\n",
    " - `dotenv.load_dotenv` - Reads the `.env` file and safely loads the environmental variables\n",
    "\n",
    " - `json` - Package to manage json-style strings/objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e38f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph core components for building agent workflows\n",
    "from langgraph.graph import StateGraph, END       # StateGraph: main graph builder, END: termination signal\n",
    "from langchain_core.runnables import RunnableLambda  # Converts functions to graph-compatible nodes\n",
    "\n",
    "# Type annotations for better code clarity and IDE support\n",
    "from typing import Dict, List, Optional, TypedDict\n",
    "\n",
    "# Utility and system packages\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Our custom utility functions from the previous notebook\n",
    "from my_functions import databricks_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0fa77b",
   "metadata": {},
   "source": [
    "### 1.3 Loading Environmental Variables\n",
    "\n",
    "Load our Azure OpenAI configuration from the secure `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddfa44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (Azure endpoints, API versions, etc.)\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2405931f",
   "metadata": {},
   "source": [
    "## 2 Defining Functions and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46826d",
   "metadata": {},
   "source": [
    "### 2.1 Classes\n",
    "\n",
    "Here, let's change the `AgentState` class a bit:\n",
    " - We need to add a few fields; Let's add a field of available tools, so we can check all the tools we have access to at any point in the graph\n",
    " - We'll aslo add a field called tool_context, for any new context the tools might give that other nodes can take advantage of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddd05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    Enhanced state container for our tool-enabled agent.\n",
    "    \n",
    "    Carries conversation data and tool information between graph nodes.\n",
    "    \"\"\"\n",
    "    # Core conversation data (from previous notebook)\n",
    "    chat_history: List[Dict[str, str]]        # Complete conversation in OpenAI format\n",
    "    output: Optional[str]                     # Most recent response\n",
    "    \n",
    "    # New fields for tool integration\n",
    "    available_tools: Optional[Dict[str, str]] # Tool names → descriptions for router\n",
    "    tool_context: Optional[str]               # Context retrieved by tools (cleared each turn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bb187e",
   "metadata": {},
   "source": [
    "### 2.2 Agent Functions with Tool Support\n",
    "\n",
    "**Three Specialized Agent Functions:**\n",
    "\n",
    "**1. Router Agent (`router_agent`)**\n",
    "- **Purpose**: Decides which tool to use based on user's query\n",
    "- **Input**: Chat history + available tools\n",
    "- **Output**: JSON decision like `{\"tool\": \"email\"}` or `{\"tool\": \"chat\"}`\n",
    "- **Key Feature**: Uses its own system prompt to focus on tool selection\n",
    "\n",
    "**2. Email Tool (`email_tool`)**  \n",
    "- **Purpose**: Retrieves email data\n",
    "- **Process**: Adds results to `tool_context`\n",
    "- **No LLM needed**: Pure data retrieval function\n",
    "\n",
    "**3. Enhanced Chat Agent (`chat_agent`)**\n",
    "- **Purpose**: Generates final response using conversation + tool context\n",
    "- **Enhancement**: Includes tool context in the prompt for informed responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d6f1b0",
   "metadata": {},
   "source": [
    "#### 2.2.1 - Router Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348e229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router_agent(state):\n",
    "    \"\"\"\n",
    "    Router agent that decides which tool to use based on the user's query.\n",
    "    \n",
    "    This agent analyzes the conversation and available tools to make an \n",
    "    intelligent routing decision.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- ROUTER AGENT NODE ---\")\n",
    "    print(f\"Analyzing user query with {len(state.get('available_tools', {}))} available tools\")\n",
    "\n",
    "    # === BUILD TOOL CATALOG FOR LLM ===\n",
    "    # Create a formatted list of available tools and their descriptions\n",
    "    tool_lines = [\n",
    "        f\"- {name}: {desc}\"\n",
    "        for name, desc in (state[\"available_tools\"] or {}).items()\n",
    "    ]\n",
    "    tool_catalog = \"\\n\".join(tool_lines) or \"none\"\n",
    "\n",
    "    # === CREATE ROUTER-SPECIFIC SYSTEM PROMPT ===\n",
    "    # The router has a specialized role: tool selection only\n",
    "    router_system_prompt = (\n",
    "        \"You are an AI router. Choose the single best tool for answering the user's \"\n",
    "        \"latest message.\\n\\n\"\n",
    "        f\"Available tools:\\n{tool_catalog}\\n\\n\"\n",
    "        \"Return ONLY a JSON object like {\\\"tool\\\": \\\"chat\\\"} or {\\\"tool\\\": \\\"email\\\"}.\"\n",
    "    )\n",
    "\n",
    "    # === PREPARE MODIFIED CHAT HISTORY ===\n",
    "    # Replace system prompts with router-specific prompt\n",
    "    # This ensures the LLM focuses on tool selection, not general chat\n",
    "    modified_chat_history = [\n",
    "        {\"role\": \"system\", \"content\": router_system_prompt}\n",
    "    ] + [\n",
    "        m for m in state[\"chat_history\"] \n",
    "        if m[\"role\"] != \"system\"  # Filter out original system prompts\n",
    "    ]\n",
    "\n",
    "    print(f\"Sending {len(modified_chat_history)} messages to LLM for routing decision\")\n",
    "\n",
    "    # === GET ROUTING DECISION FROM LLM ===\n",
    "    llm_response = databricks_llm(modified_chat_history, os.getenv(\"INSTRUCT_ENDPOINT\"))\n",
    "    \n",
    "    print(f\"LLM routing response: {llm_response}\")\n",
    "\n",
    "    # === EXTRACT JSON DECISION ===\n",
    "    # Parse JSON from LLM response (handle any extra text)\n",
    "    start = llm_response.rfind(\"{\")\n",
    "    end   = llm_response.rfind(\"}\")\n",
    "    \n",
    "    if start == -1 or end == -1:\n",
    "        # Fallback to chat if no valid JSON found\n",
    "        decision = {\"tool\": \"chat\"}\n",
    "        print(\"⚠️  No valid JSON found, defaulting to chat\")\n",
    "    else:\n",
    "        decision_json = llm_response[start : end + 1]\n",
    "        try:\n",
    "            decision = json.loads(decision_json)\n",
    "            print(f\"✓ Extracted decision: {decision}\")\n",
    "        except json.JSONDecodeError:\n",
    "            decision = {\"tool\": \"chat\"}\n",
    "            print(\"⚠️  JSON parse error, defaulting to chat\")\n",
    "\n",
    "    # === UPDATE STATE ===\n",
    "    # Store decision as JSON string for conditional edges\n",
    "    state[\"output\"] = json.dumps(decision)\n",
    "\n",
    "    print(\"--- ROUTER AGENT NODE END ---\\n\")\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95d3213",
   "metadata": {},
   "source": [
    "#### 2.2.2 - Email Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97638f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_tool(state):\n",
    "    print(\"\\n--- EMAIL TOOL NODE ---\")\n",
    "\n",
    "    with open(\"data/mock_emails.json\", \"r\") as f:\n",
    "        email_archive = json.load(f)\n",
    "\n",
    "    # Neatly format the email archive into a string\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"\"\"From: {email['sender']}\n",
    "        Subject: {email['subject']}\n",
    "        Body: {email['body']}\"\"\"\n",
    "    for email in email_archive)\n",
    "\n",
    "    print(f\"Email archive context: {context[:100]}...\")\n",
    "    # Store ONLY in scratch space – do not touch chat history\n",
    "    state[\"tool_context\"] = context\n",
    "    state[\"output\"] = \"email_context_ready\"   # optional status message\n",
    "\n",
    "    print(\"\\n--- EMAIL TOOL NODE END ---\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c68de",
   "metadata": {},
   "source": [
    "#### 2.2.3 Chat Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d54ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_agent(state):\n",
    "    \"\"\"\n",
    "    Enhanced chat agent that incorporates tool context into responses.\n",
    "    \n",
    "    This agent generates the final response using both the conversation\n",
    "    history and any additional context provided by tools.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- CHAT AGENT NODE ---\")\n",
    "    \n",
    "    # === PREPARE ENHANCED CHAT HISTORY ===\n",
    "    # Start with the original conversation\n",
    "    enhanced_chat_history = state[\"chat_history\"].copy()\n",
    "    \n",
    "    # === ADD TOOL CONTEXT IF AVAILABLE ===\n",
    "    if state.get(\"tool_context\"):\n",
    "        print(\"✓ Including tool context in conversation\")\n",
    "        # Add tool context as a user message so the AI can reference it\n",
    "        context_message = {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f\"ADDITIONAL CONTEXT FROM TOOLS:\\n{state['tool_context']}\"\n",
    "        }\n",
    "        enhanced_chat_history.append(context_message)\n",
    "    else:\n",
    "        print(\"No tool context available\")\n",
    "\n",
    "    print(f\"Sending {len(enhanced_chat_history)} messages to LLM\")\n",
    "    \n",
    "    # === GENERATE RESPONSE ===\n",
    "    reply = databricks_llm(enhanced_chat_history, os.getenv(\"CHAT_ENDPOINT\"))\n",
    "    \n",
    "    # === UPDATE STATE ===\n",
    "    # Add only the actual AI response to the permanent chat history\n",
    "    # (Don't include the temporary tool context message)\n",
    "    state[\"chat_history\"].append({\"role\": \"assistant\", \"content\": reply})\n",
    "    state[\"output\"] = reply\n",
    "\n",
    "    print(f\"✓ Generated response: {reply[:100]}...\")\n",
    "    print(\"--- CHAT AGENT NODE END ---\\n\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0582aa",
   "metadata": {},
   "source": [
    "## 3 Building the Tool-Enabled Agent Graph\n",
    "\n",
    "Now we'll create a more sophisticated graph that routes between different capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb12243",
   "metadata": {},
   "source": [
    "### 3.1 Graph with Conditional Routing\n",
    "\n",
    "**Advanced Graph Features:**\n",
    "\n",
    "**Conditional Edges:** Unlike simple edges, conditional edges make routing decisions based on the state content.\n",
    "\n",
    "**Our Graph Flow:**\n",
    "```\n",
    "START → router_agent → [decision] → email_tool → chat_agent → END\n",
    "                           ↓\n",
    "                       chat_agent → END\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Entry Point**: `router_agent` (decides the path)\n",
    "- **Route Selector**: Function that parses the router's JSON decision\n",
    "- **Conditional Logic**: Routes to `email_tool` or `chat_agent` based on decision\n",
    "- **Convergence**: Both paths eventually lead to `chat_agent` for final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62a7865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: INITIALIZE GRAPH ===\n",
    "g = StateGraph(AgentState)\n",
    "print(\"✓ Graph initialized with enhanced AgentState\")\n",
    "\n",
    "# === STEP 2: ADD NODES ===\n",
    "# Add all three types of nodes to our graph\n",
    "g.add_node(\"router_agent\", RunnableLambda(router_agent))     # Decision maker\n",
    "g.add_node(\"email_tool\",   RunnableLambda(email_tool))   # Data retriever  \n",
    "g.add_node(\"chat_agent\",   RunnableLambda(chat_agent))       # Response generator\n",
    "\n",
    "print(\"✓ Added 3 nodes: router_agent, email_tool, chat_agent\")\n",
    "\n",
    "# === STEP 3: SET ENTRY POINT ===\n",
    "# Always start with the router to make intelligent decisions\n",
    "g.set_entry_point(\"router_agent\")\n",
    "print(\"✓ Set router_agent as entry point\")\n",
    "\n",
    "# === STEP 4: DEFINE ROUTE SELECTOR FUNCTION ===\n",
    "def route_selector(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Parse the router's decision and return the target node name.\n",
    "    \n",
    "    Args:\n",
    "        state: Current agent state containing router's JSON decision\n",
    "        \n",
    "    Returns:\n",
    "        str: Node name (\"chat\" or \"email\")\n",
    "    \"\"\"\n",
    "    decision = json.loads(state[\"output\"])\n",
    "    tool_choice = decision.get(\"tool\", \"chat\")  # Default to chat if no tool specified\n",
    "    print(f\"Route selector: directing to '{tool_choice}'\")\n",
    "    return tool_choice\n",
    "\n",
    "# === STEP 5: ADD CONDITIONAL EDGES ===\n",
    "# The router's output determines which path to take\n",
    "g.add_conditional_edges(\n",
    "    \"router_agent\",        # Source node\n",
    "    route_selector,        # Function that decides the route\n",
    "    {\n",
    "        \"chat\":  \"chat_agent\",    # Direct to chat for simple queries\n",
    "        \"email\": \"email_tool\",    # Route through email tool for email queries\n",
    "    },\n",
    ")\n",
    "print(\"✓ Added conditional edges from router_agent\")\n",
    "\n",
    "# === STEP 6: ADD SIMPLE EDGES ===\n",
    "# Email tool always hands off to chat agent for final response\n",
    "g.add_edge(\"email_tool\", \"chat_agent\")\n",
    "print(\"✓ Added edge: email_tool → chat_agent\")\n",
    "\n",
    "# Both paths end at the chat agent, which then terminates\n",
    "g.add_edge(\"chat_agent\", END)\n",
    "print(\"✓ Added edge: chat_agent → END\")\n",
    "\n",
    "# === STEP 7: COMPILE THE GRAPH ===\n",
    "assistant_graph = g.compile()\n",
    "print(\"✓ Graph compiled successfully!\")\n",
    "\n",
    "print(\"\\nGraph structure:\")\n",
    "print(\"START → router_agent → [decision]\")\n",
    "print(\"                         ├─ 'chat' → chat_agent → END\")\n",
    "print(\"                         └─ 'email' → email_tool → chat_agent → END\")\n",
    "print(\"\\nThe tool-enabled agent is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c677485f",
   "metadata": {},
   "source": [
    "### 3.2 Interactive Tool-Enabled Chat\n",
    "\n",
    "**Enhanced Chat Experience:**\n",
    "\n",
    "This chat loop demonstrates the complete agent workflow:\n",
    "1. **Router Analysis**: Determines if tools are needed\n",
    "2. **Tool Execution**: Retrieves relevant data when required  \n",
    "3. **Intelligent Response**: Combines conversation context with tool data\n",
    "4. **State Management**: Clears tool context after each interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8785fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INITIALIZE ENHANCED CONVERSATION STATE ===\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI Agent. You have access to an email database if needed.\"}\n",
    "]\n",
    "\n",
    "state = AgentState(\n",
    "    chat_history=chat_history,\n",
    "    output=None,\n",
    "    # Define available tools for the router to choose from\n",
    "    available_tools={\n",
    "        \"email\": \"Search your recent e-mail archive\", \n",
    "        \"chat\": \"Continue regular conversation without tools\"\n",
    "    },\n",
    "    tool_context=None  # Will be populated by tools when needed\n",
    ")\n",
    "\n",
    "print(\"🤖 Tool-enabled AI Agent initialized!\")\n",
    "print(\"Available tools: email search, regular chat\")\n",
    "print(\"Type 'exit' to quit the conversation.\\n\")\n",
    "\n",
    "# === MAIN ENHANCED CHAT LOOP ===\n",
    "while True:\n",
    "    # === GET USER INPUT ===\n",
    "    user_text = input(\"You: \").strip()\n",
    "    \n",
    "    # === CHECK FOR EXIT ===\n",
    "    if user_text.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    if not user_text:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n📝 Processing: '{user_text}'\")\n",
    "    \n",
    "    # === ADD USER MESSAGE TO CONVERSATION ===\n",
    "    state[\"chat_history\"].append({\"role\": \"user\", \"content\": user_text})\n",
    "    \n",
    "    # === EXECUTE THE ENHANCED GRAPH ===\n",
    "    # This will:\n",
    "    # 1. Route through router_agent to decide on tools\n",
    "    # 2. Optionally use email_tool to get context\n",
    "    # 3. Generate final response with chat_agent\n",
    "    print(\"🔄 Running enhanced agent graph...\")\n",
    "    state = assistant_graph.invoke(state)\n",
    "    \n",
    "    # === CLEANUP TOOL CONTEXT ===\n",
    "    # Clear tool context after each interaction to prevent carryover\n",
    "    state[\"tool_context\"] = None\n",
    "    \n",
    "    # === DISPLAY RESPONSE ===\n",
    "    print(f\"🤖 Assistant: {state['output']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2dd496",
   "metadata": {},
   "source": [
    "## Summary: Tool-Enabled AI Agent\n",
    "\n",
    "**What We Built:**\n",
    "A sophisticated AI agent that can intelligently decide when and how to use external tools to enhance its responses.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "1. **Router Agent**: Makes intelligent decisions about tool usage\n",
    "2. **Email Tool**: Retrieves contextual information from external sources  \n",
    "3. **Enhanced Chat Agent**: Combines conversation with tool-provided context\n",
    "4. **Conditional Graph**: Routes requests based on intelligent analysis\n",
    "\n",
    "**Architecture Highlights:**\n",
    "\n",
    "**Smart Routing:** The agent doesn't just blindly use tools - it analyzes each query to determine if tools are needed.\n",
    "\n",
    "**Clean Separation:** Each component has a single responsibility:\n",
    "- Router: Decision making\n",
    "- Tools: Data retrieval  \n",
    "- Chat: Response generation\n",
    "\n",
    "**Context Management:** Tool context is added temporarily and cleaned up after each interaction.\n",
    "\n",
    "**Scalable Design:** Easy to add new tools by:\n",
    "- Writing new tools\n",
    "- Updating the `available_tools` dictionary\n",
    "- Adding new conditional routes\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Email/document search and analysis\n",
    "- Database queries with natural language\n",
    "- API integrations for external services\n",
    "- Multi-step workflows with tool chaining\n",
    "\n",
    "**Next Steps:**\n",
    "- Add more sophisticated tools (web search, calculations, file operations)\n",
    "- Implement tool chaining (using multiple tools in sequence)\n",
    "- Add conversation memory and user preferences\n",
    "- Build specialized agents for different domains\n",
    "\n",
    "**Congratulations!** 🎉 You now have a foundation for building production-ready AI agents with external tool capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ff6a0",
   "metadata": {},
   "source": [
    "## 4 - A Nod to MCP\n",
    "\n",
    "MCP stands for *Model Context Protocol*. A modern, increasingly adopted standard for enabling AI agents to connect with and invoke external tools.\n",
    "Think of MCP as a centralized hub: all the tools an AI model can access are hosted on an MCP server. Instead of embedding tool logic directly into the agent, the model simply makes API calls to the MCP server, which handles the heavy lifting. This approach offers several advantages:\n",
    "\n",
    " - Centralized management of tools and services\n",
    " - Improved monitoring and logging of tool usage\n",
    " - Offloading computation to dedicated systems, keeping the model's flow clean and efficient\n",
    " - Scalability and flexibility for enterprise-grade deployments\n",
    "\n",
    "Here's an example of the same agent we built earlier, but this time it's connected to an MCP server I spun up just for this course.\n",
    "You'll see how the agent's logic stays simple, while the MCP handles the tool orchestration behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85719d1",
   "metadata": {},
   "source": [
    "### 4.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a038206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import truststore\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cd36be",
   "metadata": {},
   "source": [
    "### 4.2 Useful Functions and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897a93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup needed for Michelin Netwrok workarounds\n",
    "for k in (\"SSL_CERT_FILE\", \"SSL_CERT_DIR\", \"REQUESTS_CA_BUNDLE\"):\n",
    "    os.environ.pop(k, None)\n",
    "truststore.inject_into_ssl()\n",
    "\n",
    "# URL of where the MCP Server is hosted\n",
    "MCP_URL = \"https://dev.d0s.michelin.com/projects/goaats_mcp/v1/mcp\"\n",
    "# Bearer Token\n",
    "TOKEN = \"dev-token-change-me\"\n",
    "\n",
    "# A function to list all tools available for this token on the MCP server\n",
    "async def list_tools():\n",
    "    client = MultiServerMCPClient(\n",
    "        connections={\n",
    "            \"server\": {\n",
    "                \"url\": MCP_URL,\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"headers\": {\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    async with client.session(\"server\") as session:\n",
    "        tools = await load_mcp_tools(session)\n",
    "        return tools\n",
    "\n",
    "# A function to call a specific tool by name\n",
    "async def call_tool(name: str, arguments: dict):\n",
    "    client = MultiServerMCPClient(\n",
    "        connections={\n",
    "            \"server\": {\n",
    "                \"url\": MCP_URL,\n",
    "                \"transport\": \"streamable_http\",\n",
    "                \"headers\": {\"Authorization\": f\"Bearer {TOKEN}\"},\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    async with client.session(\"server\") as session:\n",
    "        tools = await load_mcp_tools(session)\n",
    "        tool = next((t for t in tools if t.name == name), None)\n",
    "        if not tool:\n",
    "            return f\"Tool {name!r} not found. Available: {[t.name for t in tools]}\"\n",
    "        return await tool.ainvoke(arguments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713cd9c5",
   "metadata": {},
   "source": [
    "### 4.3 Example Usages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaaaa8a",
   "metadata": {},
   "source": [
    "#### 4.3.1 Listing Available Tools\n",
    "A quick demo of how we can store and call multiple tools from the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f202d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = await list_tools()  # Example usage\n",
    "for tool in tools:\n",
    "    print(\"----------------------------------------\")\n",
    "    print(\"Name:\", tool.name,\"\\n\")\n",
    "    print(\"Description:\", tool.description,\"\\n\")\n",
    "    print(\"Tool Arguments:\")\n",
    "    if len(tool.args_schema[\"properties\"].keys()) == 0:\n",
    "        print(\"\\tNone\")\n",
    "    else:\n",
    "        for argument in tool.args_schema[\"properties\"].keys():\n",
    "            arg_props = tool.args_schema[\"properties\"][argument]\n",
    "            print(\"\\tkey:\", argument)\n",
    "            print(\"\\tName:\", arg_props['title'])\n",
    "            print(\"\\tType:\", arg_props['type'])\n",
    "            print(\"\\tRequired:\", argument in tool.args_schema[\"required\"])\n",
    "            print(\"\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39005318",
   "metadata": {},
   "source": [
    "#### 4.3.2 Calling Specific Tool\n",
    "A quick demo of actually calling the `math_evaluator` tool, to evaluate a written expression for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await call_tool(\"math_evaluator\", {\"expression\": \"100*(1 + 0.04)^5\"})\n",
    "# Response often comes as string, or list of strings, need to jsonify it\n",
    "response = json.loads(response)\n",
    "print(\"Input to Function:\\t\\t\", response['input'])\n",
    "print(\"Normalized expression:\\t\\t\", response['normalized'])\n",
    "print(\"Evaluated expression:\\t\\t\", response['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d1c50b",
   "metadata": {},
   "source": [
    "### 4.4 Incorporating MCP Into our Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a48c6",
   "metadata": {},
   "source": [
    "#### 4.3.1 - Redefining Email Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3aaf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that it's \"async\" now, as it has API calls\n",
    "async def mcp_email_tool(state):\n",
    "    print(\"\\n--- MCP EMAIL TOOL NODE ---\")\n",
    "\n",
    "    email_archive = await call_tool(\"get_mock_emails\", {}) # Calling MCP Function\n",
    "    email_archive = [json.loads(email) for email in email_archive]\n",
    "\n",
    "    context = \"\\n\\n\".join(\n",
    "        f\"\"\"From: {email['sender']}\n",
    "        Subject: {email['subject']}\n",
    "        Body: {email['body']}\"\"\"\n",
    "    for email in email_archive)\n",
    "\n",
    "    print(f\"Email archive context: {context[:100]}...\")\n",
    "    state[\"tool_context\"] = context\n",
    "    state[\"output\"] = \"email_context_ready\"\n",
    "\n",
    "    print(\"\\n--- MCP EMAIL TOOL NODE END ---\")\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc03667",
   "metadata": {},
   "source": [
    "#### 4.3.2 - Redefining graph with our new tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe90eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: INITIALIZE GRAPH ===\n",
    "g = StateGraph(AgentState)\n",
    "\n",
    "g.add_node(\"router_agent\", RunnableLambda(router_agent)) \n",
    "g.add_node(\"email_tool\",   RunnableLambda(mcp_email_tool))   # <-- Calling MCP email tool now\n",
    "g.add_node(\"chat_agent\",   RunnableLambda(chat_agent))\n",
    "\n",
    "g.add_edge(\"email_tool\", \"chat_agent\")\n",
    "g.add_edge(\"chat_agent\", END)\n",
    "\n",
    "def route_selector(state: AgentState) -> str:\n",
    "    decision = json.loads(state[\"output\"])\n",
    "    tool_choice = decision.get(\"tool\", \"chat\")\n",
    "    return tool_choice\n",
    "\n",
    "g.add_conditional_edges(\n",
    "    \"router_agent\",\n",
    "    route_selector,\n",
    "    {\n",
    "        \"chat\":  \"chat_agent\",\n",
    "        \"email\": \"email_tool\",\n",
    "    },\n",
    ")\n",
    "\n",
    "g.set_entry_point(\"router_agent\")\n",
    "\n",
    "assistant_graph = g.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1e2d5f",
   "metadata": {},
   "source": [
    "#### 4.3.3 - Chat Loop with MCP Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fa5d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INITIALIZE ENHANCED CONVERSATION STATE ===\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI Agent. You have access to an email database if needed.\"}\n",
    "]\n",
    "\n",
    "state = AgentState(\n",
    "    chat_history=chat_history,\n",
    "    output=None,\n",
    "    available_tools={\n",
    "        \"email\": \"Search your recent e-mail archive\", \n",
    "        \"chat\": \"Continue regular conversation without tools\"\n",
    "    },\n",
    "    tool_context=None\n",
    ")\n",
    "\n",
    "print(\"🤖 Tool-enabled AI Agent initialized!\")\n",
    "print(\"Available tools: email search, regular chat\")\n",
    "print(\"Type 'exit' to quit the conversation.\\n\")\n",
    "\n",
    "while True:\n",
    "    user_text = input(\"You: \").strip()\n",
    "    \n",
    "    if user_text.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    if not user_text:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n📝 Processing: '{user_text}'\")\n",
    "\n",
    "    state[\"chat_history\"].append({\"role\": \"user\", \"content\": user_text})\n",
    "    \n",
    "    print(\"🔄 Running enhanced agent graph...\")\n",
    "    state = await assistant_graph.ainvoke(state) # <--- Only change for asynchronous execution\n",
    "\n",
    "    state[\"tool_context\"] = None\n",
    "    \n",
    "    print(f\"🤖 Assistant: {state['output']}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb44648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
