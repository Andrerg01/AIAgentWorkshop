{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1bc42c",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Welcome to the course! Here, we’ll learn everything from simple communication with LLMs to building a complex AI Agent.\n",
    "\n",
    "First, let's address the question: what is an AI Agent?\n",
    "\n",
    "*Simple answer*: An AI Agent in an AI Assistant with extra tools. These tools can be anything you program it to be, from querying data, routing requests, looking for key words, searching the internet, running Python code, ordering pizza, you name it.\n",
    "\n",
    "This course it divided into four modules:\n",
    " - **Introduction**\n",
    "   - We'll learn about the course, the Databricks environment, and a basic way to communicate with the serving LLMs\n",
    " - **Simple Chat**\n",
    "   - We'll learn to use LangGraph to create a simple chat with an LLM, which will function as a basic AI Assistant.\n",
    " - **Introducing Tools**\n",
    "   - In this module we'll introduce the first tools to our agent. We'll learn how to build a more complex graph with them, and an agent network.\n",
    " - **Final Agent**\n",
    "   - In the final module, we'll have built a complex agent with multiple tools and functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826d9ac",
   "metadata": {},
   "source": [
    "# Talking to LLMs\n",
    "If you're using Databricks, go to the \"Serving\" section in the lower-left corner to view the available models. We should have enough credits to use them comfortably for the duration of the course.\n",
    "If you're using Google Colab, I'll be providing my own personal API Key to OpenAI. Please don't abuse it.\n",
    "\n",
    "For this course we'll be using the models:\n",
    " - Databricks Serving Points:\n",
    "   - databricks-llama-4-maverick | Great for general chat\n",
    "   - databricks-meta-llama-3-1-8b-instruct | Great of following specific instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7750d3da",
   "metadata": {},
   "source": [
    "### 1 - Initial Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0a61ee",
   "metadata": {},
   "source": [
    "### 1.1 Installs\n",
    "\n",
    "* `dotenv` – loads and manages environment variables from a `.env` file.\n",
    "\n",
    "* `requests` – simple HTTP client for making API calls (e.g., to Databricks endpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c54fee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "  dotenv \\\n",
    "  requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8a7e6",
   "metadata": {},
   "source": [
    "### 1.2 Imports\n",
    "\n",
    "**What we're importing and why:**\n",
    "\n",
    "- `requests` - Basic package to make API calls\n",
    "\n",
    "- `os` - Access environment variables that store our Azure configuration\n",
    "\n",
    "- `dotenv.load_dotenv` - Reads the `.env` file and safely loads the environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "24de0f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9aa50",
   "metadata": {},
   "source": [
    "### 1.3 Set up Environmental Variables\n",
    "\n",
    "All should be declared by now on the `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b64c6908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databricks URL:  https://adb-3104295250330834.14.azuredatabricks.net/\n",
      "Databricks Token:  dapi887aeb ...\n",
      "Chat Endpoint:  databricks-llama-4-maverick\n",
      "Instruct Endpoint:  databricks-meta-llama-3-3-70b-instruct\n"
     ]
    }
   ],
   "source": [
    "# Force reload environment variables, overriding any cached values\n",
    "load_dotenv('.env', override=True)\n",
    "\n",
    "print(\"Databricks URL: \", os.getenv(\"DATABRICKS_URL\"))\n",
    "print(\"Databricks Token: \", os.getenv(\"DATABRICKS_TOKEN\")[:10], \"...\" if os.getenv(\"DATABRICKS_TOKEN\") else \"NOT FOUND\")\n",
    "print(\"Chat Endpoint: \", os.getenv(\"CHAT_ENDPOINT\"))\n",
    "print(\"Instruct Endpoint: \", os.getenv(\"INSTRUCT_ENDPOINT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289e687",
   "metadata": {},
   "source": [
    "## 2 - Basic LLM Request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51be9eb",
   "metadata": {},
   "source": [
    "### 2.1 Standalone request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "63ebbaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- RAW Response Object -----\n",
      "<Response [200]>\n",
      "\n",
      "\n",
      "----- JSON Response -----\n",
      "{'choices': [{'finish_reason': 'stop',\n",
      "              'index': 0,\n",
      "              'logprobs': None,\n",
      "              'message': {'content': \"Why couldn't the bicycle stand up by \"\n",
      "                                     'itself? Because it was two-tired!',\n",
      "                          'role': 'assistant'}}],\n",
      " 'created': 1760966682,\n",
      " 'id': 'chatcmpl_0223781a-fafe-41cd-8144-906f4ddb9e4f',\n",
      " 'model': 'meta-llama-4-maverick-040225',\n",
      " 'object': 'chat.completion',\n",
      " 'usage': {'completion_tokens': 17, 'prompt_tokens': 29, 'total_tokens': 46}}\n",
      "\n",
      "\n",
      "----- Relevant Content -----\n",
      "Why couldn't the bicycle stand up by itself? Because it was two-tired!\n"
     ]
    }
   ],
   "source": [
    "# Lets start by saving our environmental variables into actual python variables for ease of use.\n",
    "DBKS_TOKEN=os.getenv(\"DATABRICKS_TOKEN\")\n",
    "DBKS_URL=os.getenv(\"DATABRICKS_URL\")\n",
    "CHAT_ENDPOINT=os.getenv(\"CHAT_ENDPOINT\")\n",
    "\n",
    "# 'messages' is the parameter passed to the endpoint—it holds the chat history.\n",
    "# It's always a list of dictionaries with two keys:\n",
    "#   - \"role\" (who sent the message): \"user\", \"assistant\", or \"system\"\n",
    "#   - \"content\" (the actual message)\n",
    "# The \"assistant\" role refers to the LLM's responses.\n",
    "# We'll start with a basic interaction using a system prompt and a user message.\n",
    "messages = [\n",
    "  {\"role\":\"system\", \"content\":\"You're a helpful AI Assistant.\"},\n",
    "  {\"role\":\"user\", \"content\":\"Can you tell me a funny joke?\"}\n",
    "]\n",
    "\n",
    "# Headers contain authorization info and content type required by the endpoint\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {DBKS_TOKEN}\",\n",
    "    \"Content-Type\":  \"application/json\"\n",
    "}\n",
    "\n",
    "# The body includes the message history, the temperature (controls creativity), and max tokens for the response\n",
    "body = {\n",
    "    \"messages\":   messages,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\":  1000\n",
    "}\n",
    "\n",
    "# Now we can make the request\n",
    "response = requests.post(\n",
    "    f\"{DBKS_URL}/serving-endpoints/{CHAT_ENDPOINT}/invocations\",\n",
    "    headers=headers,\n",
    "    json=body\n",
    "  )\n",
    "\n",
    "# The response is a raw object—let’s inspect its contents\n",
    "print(\"----- RAW Response Object -----\")\n",
    "print(response)\n",
    "\n",
    "# Convert the response to JSON to access the content.\n",
    "# The response is stored under [\"choices\"][0][\"message\"][\"content\"].\n",
    "# By default, only one response is returned (this can be changed, but we won’t worry about it now).\n",
    "print(\"\\n\\n----- JSON Response -----\")\n",
    "pprint(response.json())\n",
    "\n",
    "# Let's extract just what we need, and print it\n",
    "print(\"\\n\\n----- Relevant Content -----\")\n",
    "print(response.json()[\"choices\"][0][\"message\"][\"content\"])\n",
    "\n",
    "# Optionally, append the assistant’s reply to the chat history\n",
    "messages.append({\"role\":\"assistant\",\"content\":response.json()[\"choices\"][0][\"message\"]})\n",
    "\n",
    "# Let's delete the variables for security\n",
    "del DBKS_TOKEN\n",
    "del DBKS_URL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf4adf",
   "metadata": {},
   "source": [
    "### 2.2 Reusable Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "533516f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LLM CALL → databricks-llama-4-maverick ===\n",
      "SYSTEM: You are a helpful assistant.\n",
      "USER: What's the capital of France?\n",
      "LLM RESPONSE: The capital of France is Paris.\n",
      "=== LLM CALL END ===\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Below is a function that has all that mess inside of it, and returns the response as a string. Easy to use, we'll be using that on the other notebooks.\n",
    "def databricks_llm(chat_history, model_endpoint, verbose=False):\n",
    "    \n",
    "    \"\"\"Call a Databricks serving endpoint that follows the OpenAI chat format.\"\"\"\n",
    "    \n",
    "    DBKS_TOKEN=os.getenv(\"DATABRICKS_TOKEN\")\n",
    "    DBKS_URL=os.getenv(\"DATABRICKS_URL\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n=== LLM CALL →\", model_endpoint, \"===\")\n",
    "        for m in chat_history:\n",
    "            print(f\"{m['role'].upper()}: {m['content']}\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {DBKS_TOKEN}\",\n",
    "        \"Content-Type\":  \"application/json\"\n",
    "    }\n",
    "    body = {\n",
    "        \"messages\":   chat_history,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\":  1000\n",
    "    }\n",
    "\n",
    "    resp = requests.post(f\"{DBKS_URL}/serving-endpoints/{model_endpoint}/invocations\", headers=headers, json=body)\n",
    "    resp.raise_for_status()\n",
    "    content = resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    if verbose: print(\"LLM RESPONSE:\", content[:300] + (\"…\" if len(content) > 300 else \"\"))\n",
    "    if verbose: print(\"=== LLM CALL END ===\")\n",
    "\n",
    "    del DBKS_TOKEN\n",
    "    del DBKS_URL\n",
    "\n",
    "    return content\n",
    "\n",
    "# Testing the function\n",
    "messages_test = [\n",
    "    {\"role\":\"system\", \"content\":\"You are a helpful assistant.\"},\n",
    "    {\"role\":\"user\", \"content\":\"What's the capital of France?\"}\n",
    "    ]\n",
    "\n",
    "print(databricks_llm(messages_test, model_endpoint=CHAT_ENDPOINT, verbose=True))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
