{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b08d9d6",
   "metadata": {},
   "source": [
    "# Simple Chat\n",
    "\n",
    "Awesome! We now know how to connect to our LLM and ask stuff from models!\n",
    "\n",
    "Now that we can do basic communication with our LLMs, we'll start on building a complex agent. Let's start with some basic configurations, learning our syntax, and establishing basic communication with an LLM through LangGraph, then we'll complicate it by adding some tools the LLM can take advantage of.\n",
    "\n",
    "Could we build a simple chat without LangGraph? Yes, easily. Could we do the rest of the course without LangGraph? Yes, but not easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a4e082",
   "metadata": {},
   "source": [
    "## 1 Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaffec17",
   "metadata": {},
   "source": [
    "### 1.1 Installs\n",
    "\n",
    "* `dotenv` ‚Äì loads and manages environment variables from a `.env` file.\n",
    "\n",
    "* `langchain-core` ‚Äì the lightweight core of LangChain, providing the `Runnable` interfaces and core abstractions.\n",
    "\n",
    "* `langgraph` ‚Äì builds stateful agent workflows as directed graphs of nodes and edges.\n",
    "\n",
    "* `requests` ‚Äì simple HTTP client for making API calls (e.g., to Databricks endpoints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94172b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install \\\n",
    "  dotenv \\\n",
    "  \"langchain-core==0.3.79\" \\\n",
    "  \"langgraph==0.2.41\" \\\n",
    "  requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc446c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Don't know how to reset  #, please run `%reset?` for details\n",
      "Don't know how to reset  vscode, please run `%reset?` for details\n"
     ]
    }
   ],
   "source": [
    "# restart kernel to use newly installed packages\n",
    "%reset -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cce7c5e",
   "metadata": {},
   "source": [
    "### 1.2 Imports\n",
    "\n",
    " - from `langgraph.graph import StateGraph, END`\n",
    "   - `StateGraph` - build and compile our node graph.\n",
    "   - `END` - sentinel that tells LangGraph where to stop.\n",
    "\n",
    " - `from langchain_core.runnables import RunnableLambda` - wraps a normal Python function so the graph can call it like any other LangChain ‚Äúrunnable.‚Äù\n",
    "\n",
    " - `from typing import Dict, List, Optional, TypedDict` - creates AgentState, a typed dictionary that documents (and type-checks) the keys we pass between nodes.\n",
    "\n",
    " - `my_functions` - Our custom useful functions\n",
    "\n",
    " - `os` - Access environment variables that store our Azure configuration\n",
    "\n",
    " - `dotenv.load_dotenv` - Reads the `.env` file and safely loads the environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d517a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph core components for building agent workflows\n",
    "from langgraph.graph import StateGraph, END       # StateGraph: main graph builder, END: termination signal\n",
    "from langchain_core.runnables import RunnableLambda  # Converts functions to graph-compatible nodes\n",
    "\n",
    "# Type annotations for better code clarity and IDE support\n",
    "from typing import Dict, List, Optional, TypedDict\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Our custom utility functions from the previous notebook\n",
    "from my_functions import databricks_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a660ee",
   "metadata": {},
   "source": [
    "### 1.3 Loading Environmental Variables\n",
    "\n",
    "Load our Azure OpenAI configuration from the secure `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e8bdf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables (Azure endpoints, API versions, etc.)\n",
    "load_dotenv('.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cd0c6",
   "metadata": {},
   "source": [
    "## 2 Defining Functions and Classes\n",
    "\n",
    "This section establishes the core components of our LangGraph agent:\n",
    "- **AgentState**: The data structure that flows between nodes\n",
    "- **Agent Functions**: The actual logic that processes the state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57d94f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0908d06d",
   "metadata": {},
   "source": [
    "### 2.1 Agent State Class\n",
    "\n",
    "**The Heart of LangGraph: State Management**\n",
    "\n",
    "The `AgentState` is a typed dictionary that carries information between all nodes in our graph. Think of it as a shared memory that gets passed around and updated by each agent/tool.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Persistence**: State persists across all nodes in the execution\n",
    "- **Updates**: Each node can read from and write to the state\n",
    "- **Type Safety**: TypedDict provides structure and IDE support\n",
    "- **Scalability**: Easy to add new fields as your agent grows\n",
    "\n",
    "**Current Fields:**\n",
    "- `chat_history`: Complete conversation in OpenAI format\n",
    "- `output`: Last response from the assistant (for easy access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f58305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state structure that flows through our agent graph\n",
    "class AgentState(TypedDict, total=False):\n",
    "    \"\"\"\n",
    "    State container for our simple chat agent.\n",
    "    \n",
    "    This state object gets passed between all nodes in the graph,\n",
    "    allowing them to share information and maintain conversation context.\n",
    "    \"\"\"\n",
    "    chat_history: List[Dict[str, str]]  # Complete conversation in OpenAI format [{\"role\": \"user\", \"content\": \"...\"}]\n",
    "    output: Optional[str]               # Most recent assistant response (for convenience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1478af66",
   "metadata": {},
   "source": [
    "### 2.2 Defining Agent Functions\n",
    "\n",
    "**Converting Business Logic into Graph Nodes**\n",
    "\n",
    "In LangGraph, every piece of functionality is a **node** - a function that:\n",
    "1. **Receives**: The current AgentState\n",
    "2. **Processes**: Performs some operation (LLM call, tool usage, logic)\n",
    "3. **Returns**: Updated AgentState with new information\n",
    "\n",
    "**Function Pattern:**\n",
    "- Input: `state` (AgentState dictionary)\n",
    "- Output: Modified `state` with updates\n",
    "- Side effects: Printing, external API calls, etc.\n",
    "\n",
    "**The chat_agent function below:**\n",
    "- Takes the conversation history from state\n",
    "- Sends it to our LLM\n",
    "- Updates the conversation with the AI's response\n",
    "- Returns the enhanced state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abf0579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_agent(state):\n",
    "    \"\"\"\n",
    "    Core chat agent node: sends conversation to LLM and updates state with response.\n",
    "    \n",
    "    This function represents our main AI agent behavior:\n",
    "    1. Takes current conversation history from state\n",
    "    2. Sends it to the LLM via our openai_llm function\n",
    "    3. Updates the conversation history with the AI's response\n",
    "    4. Stores the response in 'output' for easy access\n",
    "    \n",
    "    Args:\n",
    "        state (AgentState): Current state containing chat history\n",
    "    \n",
    "    Returns:\n",
    "        AgentState: Updated state with new AI response added\n",
    "    \"\"\"\n",
    "    print(\"\\n--- CHAT AGENT NODE ---\")\n",
    "    print(f\"Processing conversation with {len(state['chat_history'])} messages\")\n",
    "\n",
    "    # === CALL THE LLM ===\n",
    "    # Send current conversation history to our LLM function\n",
    "    # This function handles Azure authentication and API calls\n",
    "    reply = databricks_llm(\n",
    "        state[\"chat_history\"],  # Pass the complete conversation\n",
    "        os.getenv(\"CHAT_ENDPOINT\")   # Model deployment name\n",
    "    )\n",
    "    \n",
    "    print(f\"LLM Response: {reply[:100]}...\")  # Show first 100 chars\n",
    "\n",
    "    # === UPDATE THE STATE ===\n",
    "    # Add the AI's response to the conversation history\n",
    "    state[\"chat_history\"].append({\"role\": \"assistant\", \"content\": reply})\n",
    "    \n",
    "    # Store the response in output field for easy access\n",
    "    state[\"output\"] = reply\n",
    "\n",
    "    print(f\"Updated conversation now has {len(state['chat_history'])} messages\")\n",
    "    print(\"--- CHAT AGENT NODE END ---\\n\")\n",
    "\n",
    "    # Return the updated state (this gets passed to the next node, or back to caller)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0e4ca",
   "metadata": {},
   "source": [
    "## 3 Building the Agent Graph\n",
    "\n",
    "Now we assemble our agent components into a working LangGraph workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa61b2",
   "metadata": {},
   "source": [
    "### 3.1 Graph Definition\n",
    "\n",
    "**Understanding LangGraph Architecture**\n",
    "\n",
    "A LangGraph is like a flowchart for AI agents, consisting of:\n",
    "\n",
    "**Core Components:**\n",
    "- **Nodes**: Functions that process the state (our `chat_agent` function)\n",
    "- **Edges**: Connections that determine the flow between nodes\n",
    "- **Entry Point**: Which node runs first when the graph is invoked\n",
    "- **END**: Special signal that stops execution\n",
    "\n",
    "**Our Simple Graph Flow:**\n",
    "```\n",
    "START ‚Üí chat_agent ‚Üí END\n",
    "```\n",
    "\n",
    "**Why Use Graphs?**\n",
    "- **Modularity**: Each node has a single responsibility\n",
    "- **Debugging**: Easy to trace execution flow\n",
    "- **Scalability**: Simple to add new nodes (tools, validators, routers)\n",
    "- **Conditional Logic**: Can add conditional edges based on state\n",
    "- **Error Handling**: Built-in retry and error recovery patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c95af79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Graph initialized with AgentState structure\n",
      "‚úì Added 'chat_agent' node\n",
      "‚úì Set 'chat_agent' as entry point\n",
      "‚úì Added edge: chat_agent ‚Üí END\n",
      "‚úì Graph compiled successfully!\n",
      "\n",
      "Graph structure:\n",
      "START ‚Üí chat_agent ‚Üí END\n",
      "\n",
      "The graph is ready to process conversations!\n"
     ]
    }
   ],
   "source": [
    "# === STEP 1: INITIALIZE THE GRAPH ===\n",
    "# Create a StateGraph that will manage our AgentState\n",
    "g = StateGraph(AgentState)\n",
    "print(\"‚úì Graph initialized with AgentState structure\")\n",
    "\n",
    "# === STEP 2: ADD NODES ===\n",
    "# Convert our Python function into a graph node using RunnableLambda\n",
    "# The node name \"chat_agent\" is how we'll reference it in edges\n",
    "g.add_node(\"chat_agent\", RunnableLambda(chat_agent))\n",
    "print(\"‚úì Added 'chat_agent' node\")\n",
    "\n",
    "# === STEP 3: DEFINE ENTRY POINT ===\n",
    "# When someone calls .invoke() on this graph, start with the chat_agent node\n",
    "g.set_entry_point(\"chat_agent\")\n",
    "print(\"‚úì Set 'chat_agent' as entry point\")\n",
    "\n",
    "# === STEP 4: DEFINE EDGES (FLOW CONTROL) ===\n",
    "# After chat_agent finishes, go to END (stop execution)\n",
    "# This creates a simple linear flow: START ‚Üí chat_agent ‚Üí END\n",
    "g.add_edge(\"chat_agent\", END)\n",
    "print(\"‚úì Added edge: chat_agent ‚Üí END\")\n",
    "\n",
    "# === STEP 5: COMPILE THE GRAPH ===\n",
    "# Convert the graph definition into an executable workflow\n",
    "simple_chat = g.compile()\n",
    "print(\"‚úì Graph compiled successfully!\")\n",
    "\n",
    "print(\"\\nGraph structure:\")\n",
    "print(\"START ‚Üí chat_agent ‚Üí END\")\n",
    "print(\"\\nThe graph is ready to process conversations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca2c99",
   "metadata": {},
   "source": [
    "### 3.2 Interactive Chat Loop\n",
    "\n",
    "**Putting It All Together**\n",
    "\n",
    "This chat loop demonstrates how to use our compiled graph in practice:\n",
    "\n",
    "1. **Initialize**: Create starting state with system prompt\n",
    "2. **Loop**: Continuously get user input and process through graph\n",
    "3. **Update**: Add user messages to conversation history\n",
    "4. **Execute**: Run the graph with `.invoke(state)`\n",
    "5. **Display**: Show the AI's response to the user\n",
    "\n",
    "**Key Features:**\n",
    "- **Persistent Memory**: Full conversation history maintained\n",
    "- **System Prompt**: AI personality set with initial system message\n",
    "- **Exit Strategy**: Type \"exit\" to quit the conversation\n",
    "- **State Management**: Graph handles all state updates automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ba178a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè¥‚Äç‚ò†Ô∏è Pirate AI Assistant initialized!\n",
      "Type 'exit' to quit the conversation.\n",
      "\n",
      "User input received: 'hi'\n",
      "Conversation now has 2 messages\n",
      "Invoking graph...\n",
      "\n",
      "--- CHAT AGENT NODE ---\n",
      "Processing conversation with 2 messages\n",
      "LLM Response: Yer lookin' fer a swashbucklin' conversation, eh? Alright then, let's set sail fer a grand adventure...\n",
      "Updated conversation now has 3 messages\n",
      "--- CHAT AGENT NODE END ---\n",
      "\n",
      "Assistant: Yer lookin' fer a swashbucklin' conversation, eh? Alright then, let's set sail fer a grand adventure! What be bringin' ye to these fair waters?\n",
      "--------------------------------------------------\n",
      "Goodbye! May the winds fill your sails! üè¥‚Äç‚ò†Ô∏è\n"
     ]
    }
   ],
   "source": [
    "# === INITIALIZE CONVERSATION STATE ===\n",
    "# Start with a system prompt to define the AI's personality/behavior\n",
    "chat_history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant that always talks like a pirate.\"}\n",
    "]\n",
    "\n",
    "# Create initial state object with conversation history and empty output\n",
    "state = AgentState(\n",
    "    chat_history=chat_history,\n",
    "    output=None\n",
    ")\n",
    "\n",
    "print(\"üè¥‚Äç‚ò†Ô∏è Pirate AI Assistant initialized!\")\n",
    "print(\"Type 'exit' to quit the conversation.\\n\")\n",
    "\n",
    "# === MAIN CHAT LOOP ===\n",
    "while True:\n",
    "    # === GET USER INPUT ===\n",
    "    user_text = input(\"You: \").strip()\n",
    "    \n",
    "    # === CHECK FOR EXIT COMMAND ===\n",
    "    if user_text.lower() == \"exit\":\n",
    "        print(\"Goodbye! May the winds fill your sails! üè¥‚Äç‚ò†Ô∏è\")\n",
    "        break\n",
    "    \n",
    "    # Skip empty inputs\n",
    "    if not user_text:\n",
    "        continue\n",
    "    \n",
    "    print(f\"User input received: '{user_text}'\")\n",
    "    \n",
    "    # === ADD USER MESSAGE TO CONVERSATION ===\n",
    "    # Append the user's message to our conversation history\n",
    "    state[\"chat_history\"].append({\"role\": \"user\", \"content\": user_text})\n",
    "    print(f\"Conversation now has {len(state['chat_history'])} messages\")\n",
    "    \n",
    "    # === EXECUTE THE GRAPH ===\n",
    "    # This runs our entire agent workflow:\n",
    "    # 1. Calls chat_agent node with current state\n",
    "    # 2. chat_agent sends conversation to LLM\n",
    "    # 3. chat_agent updates state with AI response\n",
    "    # 4. Returns updated state\n",
    "    print(\"Invoking graph...\")\n",
    "    state = simple_chat.invoke(state)\n",
    "    \n",
    "    # === DISPLAY AI RESPONSE ===\n",
    "    # The 'output' field contains the most recent AI response\n",
    "    print(f\"Assistant: {state['output']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de9b4d4",
   "metadata": {},
   "source": [
    "## Summary: Simple Chat Agent\n",
    "\n",
    "**What We Built:**\n",
    "A basic but complete AI agent using LangGraph that maintains conversation context and responds with a pirate personality.\n",
    "\n",
    "**Key Components:**\n",
    "1. **AgentState**: Type-safe state container for conversation data\n",
    "2. **chat_agent()**: Node function that processes conversations via LLM\n",
    "3. **StateGraph**: Workflow that orchestrates the agent execution\n",
    "4. **Chat Loop**: Interactive interface for users\n",
    "\n",
    "**Why LangGraph?**\n",
    "While we could build simple chat without LangGraph, it provides:\n",
    "- **Structured Architecture**: Clear separation of concerns\n",
    "- **Scalability**: Easy to add tools, validation, routing logic\n",
    "- **Debugging**: Built-in execution tracing and state inspection\n",
    "- **Error Handling**: Retry mechanisms and failure recovery\n",
    "- **Complex Workflows**: Support for conditional logic and parallel execution\n",
    "\n",
    "**Next Steps:**\n",
    "In the following notebooks, we'll enhance this foundation by:\n",
    "- Adding tools the agent can use (web search, calculations, etc.)\n",
    "- Implementing conditional logic and routing\n",
    "- Building multi-agent systems\n",
    "- Adding memory and persistence\n",
    "\n",
    "**Foundation Complete!** üè¥‚Äç‚ò†Ô∏è\n",
    "You now have a working AI agent framework that can be extended with virtually any functionality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
